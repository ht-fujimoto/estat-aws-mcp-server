#!/usr/bin/env python3
"""
e-Stat Analysis MCP Server with Human-in-the-Loop Support
ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¤ãƒ³ã‚¶ãƒ«ãƒ¼ãƒ—å¯¾å¿œã®e-Statåˆ†æã‚µãƒ¼ãƒãƒ¼
"""

import os
import json
import asyncio
import requests
from datetime import datetime
from typing import Dict, Any, List, Optional

# ç’°å¢ƒå¤‰æ•°
ESTAT_APP_ID = os.environ.get('ESTAT_APP_ID', '320dd2fbff6974743e3f95505c9f346650ab635e')
S3_BUCKET = os.environ.get('S3_BUCKET', 'estat-data-lake')
AWS_REGION = os.environ.get('AWS_REGION', 'ap-northeast-1')

# å®šæ•°
LARGE_DATASET_THRESHOLD = 100000  # 10ä¸‡ä»¶


class EStatHITLServer:
    """Human-in-the-Loopå¯¾å¿œã®e-Statåˆ†æã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self):
        self.app_id = ESTAT_APP_ID
        self.base_url = "https://api.e-stat.go.jp/rest/3.0/app/json"
        
        # AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦åˆæœŸåŒ–ï¼‰
        try:
            import boto3
            self.s3_client = boto3.client('s3', region_name=AWS_REGION)
        except ImportError:
            self.s3_client = None
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«1: search_and_rank_datasets
    # ========================================
    
    async def search_and_rank_datasets(
        self,
        query: str,
        max_results: int = 5,
        scoring_method: str = "enhanced",
        auto_suggest: bool = True
    ) -> Dict[str, Any]:
        """
        è‡ªç„¶è¨€èªæ¤œç´¢ + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾— + ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + Top5è¿”å´
        
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        0. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µã‚¸ã‚§ã‚¹ãƒˆï¼ˆauto_suggest=Trueã®å ´åˆï¼‰
        1. getStatsList ã§æ¤œç´¢ï¼ˆ100ä»¶ï¼‰
        2. åˆæœŸã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
        3. Top 20 ã‚’é¸æŠ
        4. Top 20 ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—
        5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã¦å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        6. Top 5 ã‚’è¿”å´ï¼ˆå…¨æƒ…å ±å«ã‚€ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆä¾‹: "æ±äº¬éƒ½ã®äº¤é€šäº‹æ•…çµ±è¨ˆ"ï¼‰
            max_results: è¿”å´ã™ã‚‹æœ€å¤§ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
            scoring_method: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°æ–¹æ³•ï¼ˆ"enhanced" or "basic"ï¼‰
            auto_suggest: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        
        Returns:
            ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸçµ±è¨ˆè¡¨ãƒªã‚¹ãƒˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã€ã‚«ãƒ†ã‚´ãƒªè©³ç´°å«ã‚€ï¼‰
            ã‚µã‚¸ã‚§ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯ã€suggestions ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚å«ã‚€
        """
        print(f"\nğŸ” search_and_rank_datasets: query='{query}'")
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—0: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µã‚¸ã‚§ã‚¹ãƒˆ
            suggestions = None
            if auto_suggest:
                try:
                    from estat_enhanced_dictionary import get_keyword_suggestions, format_suggestion_message
                except ImportError:
                    from estat_keyword_dictionary import get_keyword_suggestions, format_suggestion_message
                
                keyword_suggestions = get_keyword_suggestions(query)
                if keyword_suggestions:
                    print(f"   ğŸ’¡ Found {len(keyword_suggestions)} keyword suggestions")
                    
                    # ã‚µã‚¸ã‚§ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                    suggestion_message = format_suggestion_message(keyword_suggestions)
                    
                    # ã‚µã‚¸ã‚§ã‚¹ãƒˆæƒ…å ±ã‚’è¿”å´ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‰¿èªå¾…ã¡ï¼‰
                    suggestions = {
                        "original_query": query,
                        "suggestions": keyword_suggestions,
                        "message": suggestion_message
                    }
                    
                    # ã‚µã‚¸ã‚§ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯ã€æ¤œç´¢ã‚’å®Ÿè¡Œã›ãšã«è¿”å´
                    return {
                        "success": True,
                        "has_suggestions": True,
                        "suggestions": suggestions,
                        "message": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰æ›ã®ææ¡ˆãŒã‚ã‚Šã¾ã™ã€‚å¤‰æ›ã‚’é©ç”¨ã™ã‚‹å ´åˆã¯ã€suggested_queryã§å†æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"
                    }
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: e-Stat APIå‘¼ã³å‡ºã—ï¼ˆæ¤œç´¢ï¼‰
            params = {
                "appId": self.app_id,
                "searchWord": query,
                "limit": 100  # å¤šã‚ã«å–å¾—ã—ã¦ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿
            }
            
            response = requests.get(
                f"{self.base_url}/getStatsList",
                params=params,
                timeout=30
            )
            response.raise_for_status()
            data = response.json()
            
            # çµ±è¨ˆè¡¨ãƒªã‚¹ãƒˆã‚’å–å¾—
            table_list = data.get('GET_STATS_LIST', {}).get('DATALIST_INF', {}).get('TABLE_INF', [])
            
            # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯é…åˆ—åŒ–
            if isinstance(table_list, dict):
                table_list = [table_list]
            elif not table_list:
                return {
                    "success": True,
                    "query": query,
                    "results": [],
                    "message": f"No datasets found for query '{query}'"
                }
            
            print(f"   ğŸ“Š Found {len(table_list)} datasets from search")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: åˆæœŸã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
            scored_datasets = []
            for table in table_list:
                basic_score = self._calculate_basic_score(query, table)
                
                scored_datasets.append({
                    "table": table,
                    "basic_score": basic_score
                })
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: Top 20 ã‚’é¸æŠ
            scored_datasets.sort(key=lambda x: x['basic_score'], reverse=True)
            top_20 = scored_datasets[:min(20, len(scored_datasets))]
            
            print(f"   ğŸ¯ Selected top {len(top_20)} for metadata retrieval")
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: Top 20 ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—
            if scoring_method == "enhanced":
                print(f"   ğŸ“¥ Fetching metadata for top {len(top_20)} datasets...")
                
                # ä¸¦åˆ—å‡¦ç†ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
                tasks = [
                    self._get_metadata_quick(item['table'].get('@id'))
                    for item in top_20
                ]
                metadata_list = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ã‚¢ã‚¤ãƒ†ãƒ ã«è¿½åŠ 
                for item, metadata in zip(top_20, metadata_list):
                    if isinstance(metadata, dict) and not isinstance(metadata, Exception):
                        item['metadata'] = metadata
                    else:
                        item['metadata'] = {}
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã¦å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            for item in top_20:
                if scoring_method == "enhanced":
                    enhanced_score = self._calculate_enhanced_score(
                        query,
                        item['table'],
                        item.get('metadata', {})
                    )
                    item['final_score'] = enhanced_score
                else:
                    item['final_score'] = item['basic_score']
            
            # ã‚¹ãƒ†ãƒƒãƒ—6: Top N ã‚’è¿”å´
            top_20.sort(key=lambda x: x['final_score'], reverse=True)
            top_results = top_20[:max_results]
            
            # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_results = []
            for i, item in enumerate(top_results, 1):
                table = item['table']
                metadata = item.get('metadata', {})
                
                # TITLEã¨GOV_ORGã‚’æŠ½å‡º
                title_val = table.get('TITLE', {})
                if isinstance(title_val, dict):
                    title = title_val.get('$', 'N/A')
                else:
                    title = title_val if title_val else 'N/A'
                
                gov_org_val = table.get('GOV_ORG', {})
                if isinstance(gov_org_val, dict):
                    gov_org = gov_org_val.get('$', 'N/A')
                else:
                    gov_org = gov_org_val if gov_org_val else 'N/A'
                
                # åŸºæœ¬æƒ…å ±
                result = {
                    "rank": i,
                    "score": round(item['final_score'], 3),
                    "dataset_id": table.get('@id'),
                    "title": title,
                    "gov_org": gov_org,
                    "survey_date": table.get('SURVEY_DATE', 'N/A'),
                    "open_date": table.get('OPEN_DATE', 'N/A')
                }
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’è¿½åŠ 
                if metadata:
                    result["total_records"] = metadata.get('total_records')
                    result["total_records_formatted"] = metadata.get('total_records_formatted', 'ä¸æ˜')
                    result["requires_filtering"] = metadata.get('requires_filtering')
                    
                    # ã‚«ãƒ†ã‚´ãƒªè©³ç´°ã‚’è¿½åŠ ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                    if 'categories' in metadata:
                        categories = metadata['categories']
                        result["categories"] = {}
                        for cat_id, cat_info in categories.items():
                            if isinstance(cat_info, dict):
                                result["categories"][cat_id] = {
                                    'name': cat_info.get('name', cat_id),
                                    'count': len(cat_info.get('values', [])),
                                    'sample': cat_info.get('values', [])[:5]  # æœ€åˆã®5ä»¶ã®ã¿
                                }
                            else:
                                # æ—§æ§‹é€ ã®å ´åˆ
                                result["categories"][cat_id] = cat_info
                
                formatted_results.append(result)
            
            print(f"   âœ… Returning top {len(formatted_results)} datasets with metadata")
            
            return {
                "success": True,
                "query": query,
                "total_found": len(table_list),
                "results": formatted_results,
                "message": f"Found {len(formatted_results)} relevant datasets with metadata. Please select one by dataset_id."
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _get_metadata_quick(self, dataset_id: str) -> Dict[str, Any]:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é«˜é€Ÿå–å¾—ï¼ˆç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° + ã‚«ãƒ†ã‚´ãƒªè©³ç´°ï¼‰
        
        Args:
            dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
        
        Returns:
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ï¼ˆç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã€ã‚«ãƒ†ã‚´ãƒªè©³ç´°ï¼‰
        """
        try:
            # getMetaInfo APIã‹ã‚‰å–å¾—
            params = {
                "appId": self.app_id,
                "statsDataId": dataset_id
            }
            
            response = requests.get(
                f"{self.base_url}/getMetaInfo",
                params=params,
                timeout=10
            )
            response.raise_for_status()
            meta_data = response.json()
            
            # ãƒ¡ã‚¿æƒ…å ±ã‚’è§£æ
            meta_info = meta_data.get('GET_META_INFO', {})
            metadata_inf = meta_info.get('METADATA_INF', {})
            table_inf = metadata_inf.get('TABLE_INF', {})
            
            # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
            class_inf = metadata_inf.get('CLASS_INF', {})
            
            # CLASS_OBJ ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®æ§‹é€ ï¼‰
            class_obj_list = class_inf.get('CLASS_OBJ', [])
            if not isinstance(class_obj_list, list):
                class_obj_list = [class_obj_list] if class_obj_list else []
            
            # ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã‚’è¨ˆç®—ï¼ˆæ¬¡å…ƒã®çµ„ã¿åˆã‚ã›ã‹ã‚‰ï¼‰
            total_records = 0
            
            # TOTAL_NUMBERè¦ç´ ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
            if 'TOTAL_NUMBER' in table_inf:
                try:
                    total_records = int(table_inf.get('TOTAL_NUMBER', 0))
                except (ValueError, TypeError):
                    total_records = 0
            
            # TITLEã®@noå±æ€§ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œï¼ˆé€šå¸¸ã¯ä¸æ­£ç¢ºï¼‰
            if total_records == 0:
                title_obj = table_inf.get('TITLE', {})
                if isinstance(title_obj, dict) and '@no' in title_obj:
                    try:
                        title_no = int(title_obj.get('@no', 0))
                        # @noãŒæ˜ã‚‰ã‹ã«å°ã•ã™ãã‚‹å ´åˆã¯ä½¿ç”¨ã—ãªã„
                        if title_no > 100:  # é–¾å€¤ã‚’è¨­å®š
                            total_records = title_no
                    except (ValueError, TypeError):
                        pass
            
            # æ¬¡å…ƒã®çµ„ã¿åˆã‚ã›ã‹ã‚‰è¨ˆç®—ï¼ˆæœ€ã‚‚æ­£ç¢ºï¼‰
            if total_records == 0 and class_obj_list:
                calculated_total = 1
                for class_obj in class_obj_list:
                    class_values = class_obj.get('CLASS', [])
                    if not isinstance(class_values, list):
                        class_values = [class_values] if class_values else []
                    calculated_total *= len(class_values)
                
                if calculated_total > 0:
                    total_records = calculated_total
            
            categories = {}
            for class_obj in class_obj_list:
                class_name = class_obj.get('@name', 'unknown')
                class_id = class_obj.get('@id', 'unknown')
                class_values = class_obj.get('CLASS', [])
                
                if not isinstance(class_values, list):
                    class_values = [class_values] if class_values else []
                
                # ã‚«ãƒ†ã‚´ãƒªåã®ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆå…¨ä»¶å–å¾—ï¼‰
                category_names = [
                    cv.get('@name', '') for cv in class_values
                ]
                
                # @idã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨ï¼ˆã‚ˆã‚Šè­˜åˆ¥ã—ã‚„ã™ã„ï¼‰
                categories[class_id] = {
                    'name': class_name,
                    'values': category_names  # å…¨ä»¶ä¿å­˜ï¼ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç”¨ï¼‰
                }
            
            # 10ä¸‡ä»¶åˆ¤å®š
            if total_records > 0:
                requires_filtering = total_records >= LARGE_DATASET_THRESHOLD
                formatted = f"{total_records:,}ä»¶"
            else:
                # ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãŒå–å¾—ã§ããªã„å ´åˆã¯ã€Œä¸æ˜ã€
                requires_filtering = None
                formatted = "ä¸æ˜"
            
            return {
                "total_records": total_records if total_records > 0 else None,
                "total_records_formatted": formatted,
                "requires_filtering": requires_filtering,
                "categories": categories
            }
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ä¸æ˜ã¨ã—ã¦è¿”ã™
            print(f"   âš ï¸  Metadata fetch error: {str(e)}")
            return {
                "total_records": None,
                "total_records_formatted": "ä¸æ˜",
                "requires_filtering": None,
                "categories": {}
            }
    

    def _calculate_basic_score(self, query: str, dataset: dict) -> float:
        """
        åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            dataset: çµ±è¨ˆè¡¨æƒ…å ±
        
        Returns:
            0.0 ~ 1.0 ã®ã‚¹ã‚³ã‚¢
        """
        score = 0.0
        query_keywords = [k for k in query.split() if len(k) > 1]
        
        # 1. ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒï¼ˆ25%ï¼‰
        title_val = dataset.get('TITLE', {})
        if isinstance(title_val, dict):
            title = title_val.get('$', '')
        else:
            title = title_val if title_val else ''
        
        if query_keywords:
            matches = sum(1 for keyword in query_keywords if keyword in title)
            score += 0.25 * (matches / len(query_keywords))
        
        # 2. çµ±è¨ˆåãƒ»åˆ†é¡ãƒãƒƒãƒï¼ˆ15%ï¼‰
        stats_name = dataset.get('STATISTICS_NAME', '')
        
        main_cat_val = dataset.get('MAIN_CATEGORY', {})
        if isinstance(main_cat_val, dict):
            main_cat = main_cat_val.get('$', '')
        else:
            main_cat = main_cat_val if main_cat_val else ''
        
        sub_cat_val = dataset.get('SUB_CATEGORY', {})
        if isinstance(sub_cat_val, dict):
            sub_cat = sub_cat_val.get('$', '')
        else:
            sub_cat = sub_cat_val if sub_cat_val else ''
        
        category_text = f"{stats_name} {main_cat} {sub_cat}"
        
        if query_keywords:
            cat_matches = sum(1 for keyword in query_keywords if keyword in category_text)
            score += 0.15 * (cat_matches / len(query_keywords))
        
        # 3. èª¬æ˜æ–‡ãƒãƒƒãƒï¼ˆ10%ï¼‰
        description = dataset.get('DESCRIPTION', '')
        if query_keywords and description:
            desc_matches = sum(1 for keyword in query_keywords if keyword in description)
            score += 0.1 * (desc_matches / len(query_keywords))
        
        # 4. æ›´æ–°æ—¥ã®æ–°ã—ã•ï¼ˆ15%ï¼‰
        open_date = dataset.get('OPEN_DATE', '')
        if open_date:
            try:
                date_obj = datetime.strptime(open_date, '%Y-%m-%d')
                days_old = (datetime.now() - date_obj).days
                
                if days_old <= 365:
                    freshness = 1.0
                elif days_old <= 1825:
                    freshness = 1.0 - (days_old - 365) / 1460 * 0.5
                elif days_old <= 3650:
                    freshness = 0.5 - (days_old - 1825) / 1825 * 0.5
                else:
                    freshness = 0.0
                
                score += 0.15 * freshness
            except (ValueError, TypeError):
                score += 0.075
        
        # 5. æ”¿åºœçµ„ç¹”ã®ä¿¡é ¼æ€§ï¼ˆ10%ï¼‰
        trusted_orgs = ['ç·å‹™çœ', 'è­¦å¯Ÿåº', 'å›½åœŸäº¤é€šçœ', 'åšç”ŸåŠ´åƒçœ', 'å†…é–£åºœ']
        gov_org_val = dataset.get('GOV_ORG', {})
        if isinstance(gov_org_val, dict):
            gov_org = gov_org_val.get('$', '')
        else:
            gov_org = gov_org_val if gov_org_val else ''
        
        if any(org in gov_org for org in trusted_orgs):
            score += 0.1
        
        # 6. ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ï¼ˆ5%ï¼‰
        completeness = 0
        if dataset.get('STATISTICS_NAME'):
            completeness += 1
        if dataset.get('DESCRIPTION'):
            completeness += 1
        if dataset.get('TITLE_SPEC'):
            completeness += 1
        
        score += 0.05 * (completeness / 3)
        
        return min(score, 1.0)
    
    def _calculate_enhanced_score(self, query: str, dataset: dict, metadata: dict) -> float:
        """
        å¼·åŒ–ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            dataset: çµ±è¨ˆè¡¨æƒ…å ±
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±
        
        Returns:
            0.0 ~ 1.0 ã®ã‚¹ã‚³ã‚¢
        """
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼ˆ80%ï¼‰
        basic_score = self._calculate_basic_score(query, dataset)
        score = basic_score * 0.8
        
        # 7. ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒï¼ˆ15%ï¼‰
        category_match_score = self._calculate_category_match_score(query, metadata)
        score += 0.15 * category_match_score
        
        # 8. ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã®é©åˆ‡æ€§ï¼ˆ5%ï¼‰
        data_size_score = self._calculate_data_size_score(metadata.get('total_records'))
        score += 0.05 * data_size_score
        
        return min(score, 1.0)
    
    def _calculate_category_match_score(self, query: str, metadata: dict) -> float:
        """
        ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±
        
        Returns:
            0.0 ~ 1.0 ã®ã‚¹ã‚³ã‚¢
        """
        query_keywords = [k for k in query.split() if len(k) > 1]
        if not query_keywords:
            return 0.0
        
        categories = metadata.get('categories', {})
        if not categories:
            return 0.0
        
        # å„ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®ã‚«ãƒ†ã‚´ãƒªã‚’ãƒã‚§ãƒƒã‚¯
        total_matches = 0
        for category_info in categories.values():
            # æ–°ã—ã„æ§‹é€ ã«å¯¾å¿œ
            if isinstance(category_info, dict):
                category_values = category_info.get('values', [])
            else:
                # æ—§æ§‹é€ ï¼ˆãƒªã‚¹ãƒˆï¼‰ã«ã‚‚å¯¾å¿œ
                category_values = category_info if isinstance(category_info, list) else []
            
            # ã‚«ãƒ†ã‚´ãƒªåã‚’çµåˆ
            category_text = ' '.join(category_values)
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
            matches = sum(1 for keyword in query_keywords if keyword in category_text)
            total_matches += matches
        
        # ã‚¹ã‚³ã‚¢åŒ–ï¼ˆæœ€å¤§1.0ï¼‰
        score = min(total_matches / len(query_keywords), 1.0)
        return score
    
    def _calculate_data_size_score(self, total_records: Optional[int]) -> float:
        """
        ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã®é©åˆ‡æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        çµ±è¨ˆåˆ†æã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®è±Šå¯Œã•ãŒé‡è¦
        
        Args:
            total_records: ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        
        Returns:
            0.0 ~ 1.0 ã®ã‚¹ã‚³ã‚¢
        """
        if total_records is None:
            return 0.5  # ä¸æ˜ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # çµ±è¨ˆåˆ†æã§ã¯å¤šãã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒæœ‰ç”¨
        if total_records >= 10000:
            return 1.0  # è±Šå¯Œãªãƒ‡ãƒ¼ã‚¿ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰
        elif total_records >= 1000:
            return 0.9  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿
        elif total_records >= 100:
            return 0.7  # åŸºæœ¬çš„ãªåˆ†æå¯èƒ½
        elif total_records >= 10:
            return 0.5  # é™å®šçš„ãªåˆ†æ
        else:
            return 0.3  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«2: apply_keyword_suggestionsï¼ˆæ–°è¦ï¼‰
    # ========================================
    
    def apply_keyword_suggestions_and_search(
        self,
        original_query: str,
        accepted_keywords: Dict[str, str]
    ) -> str:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰æ›ã‚’é©ç”¨ã—ã¦æ–°ã—ã„ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        
        Args:
            original_query: å…ƒã®ã‚¯ã‚¨ãƒª
            accepted_keywords: æ‰¿èªã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰æ› {å…ƒ: æ–°}
        
        Returns:
            å¤‰æ›å¾Œã®ã‚¯ã‚¨ãƒª
        """
        try:
            from estat_enhanced_dictionary import apply_keyword_suggestions
        except ImportError:
            from estat_keyword_dictionary import apply_keyword_suggestions
        
        new_query = apply_keyword_suggestions(original_query, accepted_keywords)
        print(f"   âœ… Query transformed: '{original_query}' â†’ '{new_query}'")
        
        return new_query
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«3: fetch_dataset_autoï¼ˆçµ±åˆç‰ˆï¼‰
    # ========================================
    # æ³¨: get_dataset_metadataã¯å‰Šé™¤ã•ã‚Œã€search_and_rank_datasetsã«çµ±åˆã•ã‚Œã¾ã—ãŸ
    
    async def fetch_dataset_auto(
        self,
        dataset_id: str,
        convert_to_japanese: bool = True,
        save_to_s3: bool = True
    ) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰
        
        Args:
            dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
            convert_to_japanese: ã‚³ãƒ¼ãƒ‰â†’å’Œåå¤‰æ›ã‚’å®Ÿæ–½ã™ã‚‹ã‹
            save_to_s3: S3ã«ä¿å­˜ã™ã‚‹ã‹
        
        Returns:
            å–å¾—çµæœ
        """
        print(f"\nğŸ“¥ fetch_dataset_auto: dataset_id='{dataset_id}' (auto-complete mode)")
        
        try:
            # Step 1: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’äº‹å‰ç¢ºèª
            test_params = {
                "appId": self.app_id,
                "statsDataId": dataset_id,
                "limit": 1,
                "metaGetFlg": "Y"
            }
            
            test_response = requests.get(
                f"{self.base_url}/getStatsData",
                params=test_params,
                timeout=30
            )
            test_response.raise_for_status()
            test_data = test_response.json()
            
            total_number = test_data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {}).get('RESULT_INF', {}).get('TOTAL_NUMBER', 0)
            
            print(f"   ğŸ“Š Dataset size: {total_number:,} records")
            
            # Step 2: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ãŸå–å¾—æ–¹æ³•ã®è‡ªå‹•é¸æŠ
            if total_number <= LARGE_DATASET_THRESHOLD:
                print(f"   ğŸ’¡ Small dataset - using single request")
                return await self._fetch_single_request(dataset_id, convert_to_japanese, save_to_s3)
            
            else:
                print(f"   ğŸš€ Large dataset - using complete retrieval (default behavior)")
                return await self.fetch_large_dataset_complete(
                    dataset_id=dataset_id,
                    max_records=min(total_number, 1000000),  # æœ€å¤§100ä¸‡ä»¶
                    chunk_size=100000,
                    save_to_s3=save_to_s3,
                    convert_to_japanese=convert_to_japanese
                )
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _fetch_single_request(
        self,
        dataset_id: str,
        convert_to_japanese: bool = True,
        save_to_s3: bool = True
    ) -> Dict[str, Any]:
        """
        å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        """
        try:
            start_time = datetime.now()
            
            params = {
                "appId": self.app_id,
                "statsDataId": dataset_id,
                "limit": LARGE_DATASET_THRESHOLD  # æœ€å¤§10ä¸‡ä»¶
            }
            
            response = requests.get(
                f"{self.base_url}/getStatsData",
                params=params,
                timeout=120
            )
            response.raise_for_status()
            data = response.json()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
            result_inf = stats_data.get('RESULT_INF', {})
            value_list = stats_data.get('DATA_INF', {}).get('VALUE', [])
            
            if isinstance(value_list, dict):
                value_list = [value_list]
            
            total_number = result_inf.get('TOTAL_NUMBER', 0)
            records_fetched = len(value_list)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # å®Œå…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            completeness_ratio = records_fetched / total_number if total_number > 0 else 0
            
            print(f"   ğŸ“Š Total: {total_number:,}, Fetched: {records_fetched:,} ({completeness_ratio*100:.1f}%)")
            
            # S3ã«ä¿å­˜
            s3_location = None
            if save_to_s3 and self.s3_client:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                s3_key = f"raw/data/{dataset_id}_{timestamp}.json"
                
                try:
                    self.s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=s3_key,
                        Body=json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'),
                        ContentType='application/json'
                    )
                    s3_location = f"s3://{S3_BUCKET}/{s3_key}"
                    print(f"   âœ… Saved to: {s3_location}")
                except Exception as e:
                    print(f"   âš ï¸  S3 save failed: {str(e)}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            sample_data = value_list[:5] if len(value_list) > 5 else value_list
            
            print(f"   âœ… Fetched {records_fetched:,} records in {processing_time:.1f}s")
            
            return {
                "success": True,
                "dataset_id": dataset_id,
                "records_fetched": records_fetched,
                "expected_records": total_number,
                "completeness_ratio": completeness_ratio,
                "processing_time": f"{processing_time:.1f}ç§’",
                "sample_data": sample_data,
                "s3_location": s3_location,
                "next_action": "transform_to_parquet",
                "message": f"Successfully fetched {records_fetched:,} records (100.0% complete)",
                "note": "Small dataset - complete retrieval in single request"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _fetch_complete_data_by_chunks(
        self,
        dataset_id: str,
        categories: dict,
        expected_total: int
    ) -> list:
        """
        æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚‹å®Œå…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
        
        Args:
            dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
            categories: ã‚«ãƒ†ã‚´ãƒªæƒ…å ±
            expected_total: æœŸå¾…ã•ã‚Œã‚‹ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
        
        Returns:
            å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        print(f"   ğŸ“… Starting time-series chunked retrieval...")
        
        # æ™‚é–“è»¸ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šï¼ˆé€šå¸¸ã¯cat03ãŒå¹´åº¦ï¼‰
        time_category = None
        time_values = []
        
        for cat_id, cat_info in categories.items():
            if isinstance(cat_info, dict):
                values = cat_info.get('values', [])
                category_name = cat_info.get('name', '')
                
                # å¹´åº¦ã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                if values and len(values) > 10:  # æ™‚ç³»åˆ—ã¯é€šå¸¸å¤šãã®å€¤ã‚’æŒã¤
                    year_like_count = 0
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å¹´åº¦æ–‡å­—åˆ—ï¼ˆ"1960å¹´", "2020å¹´"ãªã©ï¼‰
                    for val in values[:10]:
                        if isinstance(val, str):
                            # "YYYYå¹´"ãƒ‘ã‚¿ãƒ¼ãƒ³
                            if val.endswith('å¹´') and val[:-1].isdigit():
                                year = int(val[:-1])
                                if 1900 <= year <= 2030:
                                    year_like_count += 1
                            # "ï¼ˆæ¦‚ç®—ï¼‰_YYYYå¹´"ãƒ‘ã‚¿ãƒ¼ãƒ³
                            elif 'å¹´' in val and any(part.isdigit() and 1900 <= int(part) <= 2030 for part in val.split('_') if part.replace('å¹´', '').isdigit()):
                                year_like_count += 1
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚«ãƒ†ã‚´ãƒªåã«å¹´åº¦ãƒ»æ™‚é–“ã‚’ç¤ºã™æ–‡å­—ãŒå«ã¾ã‚Œã‚‹
                    time_keywords = ['å¹´', 'æ™‚é–“', 'æœŸé–“', 'å¹´åº¦', 'æ™‚ç³»åˆ—']
                    if any(keyword in category_name for keyword in time_keywords):
                        year_like_count += 5  # ãƒœãƒ¼ãƒŠã‚¹ç‚¹
                    
                    if year_like_count >= 5:  # å¹´åº¦ã‚‰ã—ã„
                        time_category = cat_id
                        time_values = values
                        print(f"   ğŸ“… Detected time category: {category_name}")
                        break
        
        if not time_category:
            print(f"   âš ï¸ No time-series category found for chunking")
            return []
        
        print(f"   ğŸ“… Using time category '{time_category}' with {len(time_values)} periods")
        
        # å¹´åº¦åˆ¥ã«åˆ†å‰²å–å¾—
        all_chunked_records = []
        successful_chunks = 0
        failed_chunks = 0
        
        # å¹´åº¦å€¤ã‚’å‡¦ç†ï¼ˆ"1960å¹´" â†’ "1960"ã®ã‚ˆã†ãªå¤‰æ›ï¼‰
        processed_years = []
        for val in time_values:
            if isinstance(val, str):
                if val.endswith('å¹´'):
                    year_str = val[:-1]
                    if year_str.isdigit():
                        processed_years.append((year_str, int(year_str)))
                elif 'å¹´' in val:
                    # "ï¼ˆæ¦‚ç®—ï¼‰_2022å¹´"ã®ã‚ˆã†ãªè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                    for part in val.split('_'):
                        if part.endswith('å¹´') and part[:-1].isdigit():
                            year_str = part[:-1]
                            processed_years.append((year_str, int(year_str)))
                            break
        
        # å¹´åº¦ã‚’é€†é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„å¹´åº¦ã‹ã‚‰å–å¾—ï¼‰
        sorted_years = sorted(processed_years, key=lambda x: x[1], reverse=True)
        
        print(f"   ğŸ“… Processing {len(sorted_years)} years: {sorted_years[0][0]} to {sorted_years[-1][0]}")
        
        for i, (year_code, year_num) in enumerate(sorted_years):
            try:
                chunk_params = {
                    "appId": self.app_id,
                    "statsDataId": dataset_id,
                    f"cd{time_category}": year_code,
                    "limit": 10000  # å¹´åº¦åˆ¥ãªã‚‰ååˆ†ãªé‡
                }
                
                chunk_response = requests.get(
                    f"{self.base_url}/getStatsData",
                    params=chunk_params,
                    timeout=30
                )
                chunk_response.raise_for_status()
                chunk_data = chunk_response.json()
                
                chunk_values = chunk_data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {}).get('DATA_INF', {}).get('VALUE', [])
                
                if isinstance(chunk_values, dict):
                    chunk_values = [chunk_values]
                
                if chunk_values:
                    all_chunked_records.extend(chunk_values)
                    successful_chunks += 1
                    print(f"   âœ… Year {year_num}: {len(chunk_values)} records")
                else:
                    print(f"   âšª Year {year_num}: No data")
                
                # é€²æ—è¡¨ç¤ºï¼ˆ10å¹´ã”ã¨ï¼‰
                if (i + 1) % 10 == 0:
                    current_total = len(all_chunked_records)
                    progress = (current_total / expected_total * 100) if expected_total > 0 else 0
                    print(f"   ğŸ“Š Progress: {i+1}/{len(sorted_years)} years, {current_total:,} records ({progress:.1f}%)")
                
                # æ—©æœŸçµ‚äº†æ¡ä»¶ï¼ˆæœŸå¾…å€¤ã®95%ã«é”ã—ãŸå ´åˆï¼‰
                if len(all_chunked_records) >= expected_total * 0.95:
                    print(f"   ğŸ¯ Reached 95% of expected data ({len(all_chunked_records):,}/{expected_total:,})")
                    break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆå°‘ã—å¾…æ©Ÿï¼‰
                if i % 5 == 4:  # 5å›ã”ã¨ã«å¾…æ©Ÿ
                    await asyncio.sleep(0.5)
                    
            except Exception as e:
                failed_chunks += 1
                print(f"   âŒ Year {year_num} failed: {str(e)}")
                
                # é€£ç¶šå¤±æ•—ãŒå¤šã„å ´åˆã¯ä¸­æ–­
                if failed_chunks >= 5:
                    print(f"   âš ï¸ Too many failures, stopping chunked retrieval")
                    break
                
                continue
        
        # é‡è¤‡é™¤å»
        if all_chunked_records:
            print(f"   ğŸ”„ Deduplicating chunked data...")
            unique_chunked_records = self._deduplicate_records(all_chunked_records)
            
            print(f"   ğŸ“Š Chunked retrieval summary:")
            print(f"      - Successful chunks: {successful_chunks}")
            print(f"      - Failed chunks: {failed_chunks}")
            print(f"      - Raw records: {len(all_chunked_records):,}")
            print(f"      - Unique records: {len(unique_chunked_records):,}")
            print(f"      - Completeness: {len(unique_chunked_records)/expected_total*100:.1f}%")
            
            return unique_chunked_records
        
        return []
    
    def _deduplicate_records(self, records: list) -> list:
        """
        ãƒ¬ã‚³ãƒ¼ãƒ‰ã®é‡è¤‡ã‚’é™¤å»
        
        Args:
            records: ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        
        Returns:
            é‡è¤‡é™¤å»ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        """
        if not records:
            return records
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¸€æ„ã®ã‚­ãƒ¼ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        seen_keys = set()
        unique_records = []
        
        for record in records:
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆå…¨å±æ€§ã®çµ„ã¿åˆã‚ã›ï¼‰
            if isinstance(record, dict):
                # è¾æ›¸ã®ã‚­ãƒ¼ã¨å€¤ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ä¸€æ„ã‚­ãƒ¼ã‚’ä½œæˆ
                key_parts = []
                for k in sorted(record.keys()):
                    if k != '$':  # å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯é™¤å¤–
                        key_parts.append(f"{k}:{record.get(k, '')}")
                
                unique_key = "|".join(key_parts)
                
                if unique_key not in seen_keys:
                    seen_keys.add(unique_key)
                    unique_records.append(record)
        
        return unique_records
    
    def _merge_chunked_data(self, original_data: dict, chunked_records: list) -> dict:
        """åˆ†å‰²å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ãƒãƒ¼ã‚¸"""
        merged_data = original_data.copy()
        
        # VALUEéƒ¨åˆ†ã‚’ç½®ãæ›ãˆ
        if 'GET_STATS_DATA' in merged_data:
            if 'STATISTICAL_DATA' in merged_data['GET_STATS_DATA']:
                if 'DATA_INF' in merged_data['GET_STATS_DATA']['STATISTICAL_DATA']:
                    merged_data['GET_STATS_DATA']['STATISTICAL_DATA']['DATA_INF']['VALUE'] = chunked_records
        
        return merged_data
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«4.5: fetch_large_dataset_complete (æ–°æ©Ÿèƒ½)
    # ========================================
    
    async def fetch_large_dataset_complete(
        self,
        dataset_id: str,
        max_records: int = 1000000,
        chunk_size: int = 100000,
        save_to_s3: bool = True,
        convert_to_japanese: bool = True
    ) -> Dict[str, Any]:
        """
        å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Œå…¨å–å¾—ï¼ˆæœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å–å¾—ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ç‰ˆï¼‰
        
        MCPã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶é™ï¼ˆç´„25ç§’ï¼‰ã‚’è€ƒæ…®ã—ã€æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å–å¾—ã€‚
        å®Œå…¨ãªåˆ†å‰²å–å¾—ã«ã¯ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½¿ç”¨ã‚’æ¨å¥¨ã€‚
        
        Args:
            dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
            max_records: å–å¾—ã™ã‚‹æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ä¸‡ä»¶ï¼‰
            chunk_size: 1å›ã‚ãŸã‚Šã®å–å¾—ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ä¸‡ä»¶ï¼‰
            save_to_s3: S3ã«ä¿å­˜ã™ã‚‹ã‹
            convert_to_japanese: ã‚³ãƒ¼ãƒ‰â†’å’Œåå¤‰æ›ã‚’å®Ÿæ–½ã™ã‚‹ã‹
        
        Returns:
            å–å¾—çµæœï¼ˆæœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã¨é€²è¡ŒçŠ¶æ³ï¼‰
        """
        print(f"\nğŸ“¥ fetch_large_dataset_complete: dataset_id='{dataset_id}', max_records={max_records:,}")
        
        try:
            # Step 1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’ç¢ºèª
            print("   ğŸ” Getting metadata...")
            meta_response = requests.get(
                f"{self.base_url}/getMetaInfo",
                params={"appId": self.app_id, "statsDataId": dataset_id},
                timeout=30
            )
            meta_response.raise_for_status()
            meta_data = meta_response.json()
            
            overall_total = meta_data.get('GET_META_INFO', {}).get('METADATA_INF', {}).get('TABLE_INF', {}).get('OVERALL_TOTAL_NUMBER', 0)
            
            # Step 2: å®Ÿéš›ã®ç·æ•°ã‚’APIã§ç¢ºèª
            test_response = requests.get(
                f"{self.base_url}/getStatsData",
                params={"appId": self.app_id, "statsDataId": dataset_id, "limit": 1, "metaGetFlg": "Y"},
                timeout=30
            )
            test_response.raise_for_status()
            test_data = test_response.json()
            
            actual_total = test_data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {}).get('RESULT_INF', {}).get('TOTAL_NUMBER', 0)
            
            print(f"   ğŸ“Š Metadata total: {overall_total:,}, Actual total: {actual_total:,}")
            
            # å–å¾—å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’æ±ºå®š
            target_records = min(actual_total, max_records)
            
            if target_records <= chunk_size:
                print(f"   ğŸ’¡ Small dataset ({target_records:,} records) - using single request")
                return await self.fetch_dataset_auto(dataset_id, save_to_s3, convert_to_japanese)
            
            # Step 3: æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
            total_chunks = (target_records + chunk_size - 1) // chunk_size
            print(f"   ğŸ”„ Fetching first chunk: {chunk_size:,} records")
            print(f"   âš ï¸  Note: Due to MCP timeout limits, only first chunk will be retrieved")
            print(f"   ğŸ’¡ Total chunks needed: {total_chunks}")
            
            start_time = datetime.now()
            
            # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            params = {
                "appId": self.app_id,
                "statsDataId": dataset_id,
                "limit": chunk_size,
                "startPosition": 1
            }
            
            chunk_response = requests.get(
                f"{self.base_url}/getStatsData",
                params=params,
                timeout=60
            )
            chunk_response.raise_for_status()
            chunk_data = chunk_response.json()
            
            chunk_values = chunk_data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {}).get('DATA_INF', {}).get('VALUE', [])
            
            if isinstance(chunk_values, dict):
                chunk_values = [chunk_values]
            
            print(f"      âœ… Retrieved {len(chunk_values):,} records")
            
            # Step 4: S3ã«ä¿å­˜ï¼ˆæœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿ï¼‰
            s3_location = None
            if save_to_s3 and self.s3_client:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                s3_key = f"raw/data/{dataset_id}_chunk_001_{timestamp}.json"
                
                try:
                    self.s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=s3_key,
                        Body=json.dumps(chunk_data, ensure_ascii=False, indent=2).encode('utf-8'),
                        ContentType='application/json'
                    )
                    s3_location = f"s3://{S3_BUCKET}/{s3_key}"
                    print(f"   âœ… Saved chunk 1 to: {s3_location}")
                except Exception as e:
                    print(f"   âš ï¸  S3 save failed: {str(e)}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
            sample_data = chunk_values[:5] if len(chunk_values) > 5 else chunk_values
            
            return {
                "success": True,
                "dataset_id": dataset_id,
                "metadata_total": overall_total,
                "actual_total": actual_total,
                "target_records": target_records,
                "chunk_size": chunk_size,
                "total_chunks_needed": total_chunks,
                "chunks_retrieved": 1,
                "records_in_chunk": len(chunk_values),
                "completeness": f"{len(chunk_values)/target_records*100:.1f}%",
                "processing_time": f"{processing_time:.1f}ç§’",
                "sample_data": sample_data,
                "s3_location": s3_location,
                "next_action": "Use Python script for complete retrieval",
                "recommendation": f"For complete data retrieval of {target_records:,} records, use the standalone Python script 'fetch_{dataset_id}_chunked.py' to avoid MCP timeout limits",
                "message": f"Retrieved first chunk ({len(chunk_values):,} records). Total {total_chunks} chunks needed for complete dataset.",
                "warning": "MCP timeout limit prevents full retrieval. Use standalone script for complete data."
            }
                    
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "dataset_id": dataset_id,
                "suggestion": "Try reducing max_records or using fetch_dataset_filtered with specific filters"
            }

    # ========================================
    # ãƒ„ãƒ¼ãƒ«4: fetch_dataset_filtered (ä¿®æ­£ç‰ˆ)
    # ========================================
    
    async def fetch_dataset_filtered(
        self,
        dataset_id: str,
        filters: Dict[str, str],
        convert_to_japanese: bool = True,
        save_to_s3: bool = True
    ) -> Dict[str, Any]:
        """
        ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šã§ã®çµã‚Šè¾¼ã¿å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰
        
        Args:
            dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
            filters: ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ï¼ˆä¾‹: {"area": "13000", "cat01": "A1101", "time": "2020"}ï¼‰
            convert_to_japanese: ã‚³ãƒ¼ãƒ‰â†’å’Œåå¤‰æ›ã‚’å®Ÿæ–½ã™ã‚‹ã‹
            save_to_s3: S3ã«ä¿å­˜ã™ã‚‹ã‹
        
        Returns:
            å–å¾—çµæœ
        """
        print(f"\nğŸ“¥ fetch_dataset_filtered: dataset_id='{dataset_id}', filters={filters}")
        
        try:
            # Step 1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ã‚’æ¤œè¨¼
            print("   ğŸ” Validating filters against metadata...")
            meta_response = requests.get(
                f"{self.base_url}/getMetaInfo",
                params={"appId": self.app_id, "statsDataId": dataset_id},
                timeout=30
            )
            meta_response.raise_for_status()
            meta_data = meta_response.json()
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’æŠ½å‡º
            class_objs = meta_data.get('GET_META_INFO', {}).get('METADATA_INF', {}).get('CLASS_INF', {}).get('CLASS_OBJ', [])
            available_categories = {}
            
            for class_obj in class_objs:
                cat_id = class_obj.get('@id')
                cat_name = class_obj.get('@name')
                classes = class_obj.get('CLASS', [])
                
                if isinstance(classes, dict):
                    classes = [classes]
                
                available_codes = []
                code_to_name = {}
                
                for cls in classes:
                    code = cls.get('@code')
                    name = cls.get('@name')
                    if code and name:
                        available_codes.append(code)
                        code_to_name[code] = name
                
                available_categories[cat_id] = {
                    'name': cat_name,
                    'codes': available_codes,
                    'code_to_name': code_to_name
                }
            
            print(f"   ğŸ“‹ Available categories: {list(available_categories.keys())}")
            
            # Step 2: ãƒ•ã‚£ãƒ«ã‚¿ã‚’æ¤œè¨¼ãƒ»å¤‰æ›
            validated_filters = {}
            filter_info = {}
            
            for filter_key, filter_value in filters.items():
                if filter_key in available_categories:
                    cat_info = available_categories[filter_key]
                    
                    # å€¤ãŒæ—¥æœ¬èªã®å ´åˆã€ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
                    if filter_value in cat_info['code_to_name'].values():
                        # æ—¥æœ¬èªåã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                        for code, name in cat_info['code_to_name'].items():
                            if name == filter_value:
                                validated_filters[f"cd{filter_key.title()}"] = code
                                filter_info[filter_key] = name
                                print(f"   âœ… {filter_key}: '{filter_value}' â†’ code '{code}'")
                                break
                    elif filter_value in cat_info['codes']:
                        # æ—¢ã«ã‚³ãƒ¼ãƒ‰ã®å ´åˆ
                        validated_filters[f"cd{filter_key.title()}"] = filter_value
                        filter_info[filter_key] = cat_info['code_to_name'].get(filter_value, filter_value)
                        print(f"   âœ… {filter_key}: code '{filter_value}' â†’ '{filter_info[filter_key]}'")
                    else:
                        print(f"   âš ï¸  {filter_key}: '{filter_value}' not found in available codes")
                        print(f"      Available codes: {cat_info['codes'][:10]}...")
                        # éƒ¨åˆ†ãƒãƒƒãƒã‚’è©¦è¡Œ
                        partial_matches = [code for code in cat_info['codes'] if filter_value in code]
                        if partial_matches:
                            best_match = partial_matches[0]
                            validated_filters[f"cd{filter_key.title()}"] = best_match
                            filter_info[filter_key] = cat_info['code_to_name'].get(best_match, best_match)
                            print(f"   ğŸ”„ Using partial match: '{best_match}' â†’ '{filter_info[filter_key]}'")
                        else:
                            return {
                                "success": False,
                                "error": f"Filter value '{filter_value}' not found for category '{filter_key}'",
                                "available_codes": cat_info['codes'][:20],
                                "suggestion": f"Use one of the available codes for {filter_key}"
                            }
                else:
                    print(f"   âš ï¸  Category '{filter_key}' not found in metadata")
                    return {
                        "success": False,
                        "error": f"Category '{filter_key}' not found in dataset metadata",
                        "available_categories": list(available_categories.keys()),
                        "suggestion": "Use one of the available category names"
                    }
            
            # Step 3: ãƒ‡ãƒ¼ã‚¿å–å¾—
            start_time = datetime.now()
            
            params = {
                "appId": self.app_id,
                "statsDataId": dataset_id,
                "limit": LARGE_DATASET_THRESHOLD,  # æœ€å¤§10ä¸‡ä»¶
                "metaGetFlg": "Y"  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—
            }
            
            # æ¤œè¨¼æ¸ˆã¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ 
            params.update(validated_filters)
            
            print(f"   ğŸ”„ Fetching data with params: {params}")
            
            response = requests.get(
                f"{self.base_url}/getStatsData",
                params=params,
                timeout=120
            )
            response.raise_for_status()
            data = response.json()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
            result_inf = stats_data.get('RESULT_INF', {})
            value_list = stats_data.get('DATA_INF', {}).get('VALUE', [])
            
            if isinstance(value_list, dict):
                value_list = [value_list]
            
            records_fetched = len(value_list)
            total_available = result_inf.get('TOTAL_NUMBER', 0)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            print(f"   ğŸ“Š Total available: {total_available:,}, Fetched: {records_fetched:,}")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãŒæ­£ã—ãé©ç”¨ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
            if records_fetched == 0:
                return {
                    "success": False,
                    "error": "No data returned with the specified filters",
                    "filters_applied": filter_info,
                    "total_available": total_available,
                    "suggestion": "Try different filter values or remove some filters"
                }
            
            # S3ã«ä¿å­˜
            s3_location = None
            if save_to_s3 and self.s3_client:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                s3_key = f"raw/data/{dataset_id}_filtered_{timestamp}.json"
                
                try:
                    self.s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=s3_key,
                        Body=json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'),
                        ContentType='application/json'
                    )
                    s3_location = f"s3://{S3_BUCKET}/{s3_key}"
                    print(f"   âœ… Saved to: {s3_location}")
                except Exception as e:
                    print(f"   âš ï¸  S3 save failed: {str(e)}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            sample_data = value_list[:5] if len(value_list) > 5 else value_list
            
            print(f"   âœ… Successfully fetched {records_fetched:,} records in {processing_time:.1f}s")
            
            return {
                "success": True,
                "dataset_id": dataset_id,
                "filters_applied": filter_info,
                "records_fetched": records_fetched,
                "total_available": total_available,
                "filter_effectiveness": f"{records_fetched/total_available*100:.1f}%" if total_available > 0 else "N/A",
                "processing_time": f"{processing_time:.1f}ç§’",
                "sample_data": sample_data,
                "s3_location": s3_location,
                "next_action": "transform_to_parquet",
                "message": f"Successfully fetched {records_fetched:,} records with filters (filtered from {total_available:,} total records)"
            }
            
        except Exception as e:
            return {
                "success": False, 
                "error": str(e),
                "dataset_id": dataset_id,
                "filters": filters,
                "suggestion": "Check filter values and dataset_id, or try fetch_dataset_auto for smaller datasets"
            }
    
    # ========================================
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # ========================================
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«5: transform_to_parquet
    # ========================================
    
    async def transform_to_parquet(
        self,
        s3_json_path: str,
        data_type: str,
        output_prefix: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        JSONãƒ‡ãƒ¼ã‚¿ã‚’Parquetå½¢å¼ã«å¤‰æ›ã—ã¦S3ã«ä¿å­˜
        
        Args:
            s3_json_path: S3ä¸Šã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            data_type: ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥
            output_prefix: å‡ºåŠ›å…ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            å¤‰æ›çµæœ
        """
        print(f"\nğŸ”„ transform_to_parquet: {s3_json_path}")
        
        try:
            import pandas as pd
            import pyarrow as pa
            import pyarrow.parquet as pq
            from io import BytesIO
            
            if not self.s3_client:
                return {"success": False, "error": "S3 client not available"}
            
            # S3ãƒ‘ã‚¹ã‚’è§£æ
            if s3_json_path.startswith('s3://'):
                path_parts = s3_json_path[5:].split('/', 1)
                bucket = path_parts[0]
                key = path_parts[1]
            else:
                bucket = S3_BUCKET
                key = s3_json_path
            
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            data = json.loads(response['Body'].read())
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
            data_inf = stats_data.get('DATA_INF', {})
            values = data_inf.get('VALUE', [])
            
            if not values:
                return {"success": False, "error": "No data found in JSON"}
            
            # DataFrameã«å¤‰æ›
            records = []
            dataset_id = key.split('/')[-1].split('_')[0]
            
            for value in values:
                # å€¤ã‚’å–å¾—ï¼ˆ'-'ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯Noneã«å¤‰æ›ï¼‰
                raw_value = value.get('$', '0')
                try:
                    numeric_value = float(raw_value) if raw_value and raw_value != '-' else None
                except (ValueError, TypeError):
                    numeric_value = None
                
                record = {
                    'stats_data_id': dataset_id,
                    'value': numeric_value,
                    'unit': value.get('@unit', ''),
                    'updated_at': datetime.now()
                }
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                if data_type == 'population':
                    record['year'] = int(value.get('@time', '2020'))
                    record['region_code'] = value.get('@cat01', '')
                    record['region_name'] = ''
                    record['category'] = value.get('@cat02', '')
                elif data_type == 'economy':
                    record['year'] = int(value.get('@time', '2020'))
                    record['quarter'] = 1
                    record['region_code'] = value.get('@area', '')
                    record['indicator'] = value.get('@cat01', '')
                elif data_type == 'education':
                    record['year'] = int(value.get('@time', '2020'))
                    record['region_code'] = value.get('@area', '')
                    record['school_type'] = value.get('@cat01', '')
                    record['metric'] = value.get('@cat02', '')
                else:
                    # æ±ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    record['year'] = int(value.get('@time', '2020'))
                    record['region_code'] = value.get('@area', '')
                    record['category'] = value.get('@cat01', '')
                
                records.append(record)
            
            df = pd.DataFrame(records)
            
            # Parquetã«å¤‰æ›
            table = pa.Table.from_pandas(df)
            
            # å‡ºåŠ›ãƒ‘ã‚¹ã‚’æ±ºå®š
            if output_prefix:
                parquet_key = f"{output_prefix}/{data_type}/{dataset_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
            else:
                parquet_key = key.replace('raw/data/', 'processed/').replace('.json', '.parquet')
            
            # S3ã«ä¿å­˜
            buffer = BytesIO()
            pq.write_table(table, buffer)
            buffer.seek(0)
            
            self.s3_client.put_object(
                Bucket=bucket,
                Key=parquet_key,
                Body=buffer.getvalue(),
                ContentType='application/octet-stream'
            )
            
            s3_parquet_path = f"s3://{bucket}/{parquet_key}"
            
            print(f"   âœ… Converted {len(records)} records to Parquet")
            
            return {
                "success": True,
                "source_path": s3_json_path,
                "target_path": s3_parquet_path,
                "records_processed": len(records),
                "data_type": data_type,
                "message": f"Successfully converted {len(records)} records to Parquet format"
            }
            
        except ImportError as e:
            return {"success": False, "error": f"Required libraries not available: {str(e)}"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«6: load_to_iceberg
    # ========================================
    
    async def load_to_iceberg(
        self,
        table_name: str,
        s3_parquet_path: str,
        create_if_not_exists: bool = True
    ) -> Dict[str, Any]:
        """
        Parquetãƒ‡ãƒ¼ã‚¿ã‚’Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«æŠ•å…¥ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
            s3_parquet_path: S3ä¸Šã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            create_if_not_exists: ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã«ä½œæˆã™ã‚‹ã‹
        
        Returns:
            æŠ•å…¥çµæœ
        """
        print(f"\nğŸ“Š load_to_iceberg: {table_name}")
        
        try:
            import boto3
            
            athena_client = boto3.client('athena', region_name=AWS_REGION)
            glue_client = boto3.client('glue', region_name=AWS_REGION)
            database = 'estat_db'
            output_location = f's3://{S3_BUCKET}/athena-results/'
            
            # S3ãƒ‘ã‚¹ã‚’è§£æ
            if s3_parquet_path.startswith('s3://'):
                path_parts = s3_parquet_path[5:].split('/', 1)
                bucket = path_parts[0]
                parquet_key = path_parts[1]
            else:
                bucket = S3_BUCKET
                parquet_key = s3_parquet_path
            
            # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å­˜åœ¨ç¢ºèªã¨ä½œæˆ
            print(f"   ğŸ” Checking database: {database}")
            try:
                glue_client.get_database(Name=database)
                print(f"   âœ… Database {database} exists")
            except glue_client.exceptions.EntityNotFoundException:
                print(f"   ğŸ”„ Creating database: {database}")
                try:
                    glue_client.create_database(
                        DatabaseInput={
                            'Name': database,
                            'Description': 'e-Stat analysis database for statistical data'
                        }
                    )
                    print(f"   âœ… Database {database} created")
                except Exception as db_error:
                    print(f"   âš ï¸ Database creation failed: {str(db_error)}")
                    # Athenaã§ä½œæˆã‚’è©¦è¡Œ
                    create_db_query = f"CREATE DATABASE IF NOT EXISTS {database}"
                    db_result = await self._execute_athena_query(athena_client, create_db_query, "default", output_location)
                    if not db_result[0]:
                        return {"success": False, "error": f"Failed to create database: {db_result[1]}"}
            
            # 2. ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèª
            print(f"   ğŸ” Checking table: {table_name}")
            table_exists = False
            try:
                glue_client.get_table(DatabaseName=database, Name=table_name)
                table_exists = True
                print(f"   âœ… Table {table_name} exists")
            except glue_client.exceptions.EntityNotFoundException:
                print(f"   â„¹ï¸ Table {table_name} does not exist")
            
            # 3. ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰ã¾ãŸã¯æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢
            if table_exists:
                print(f"   ğŸ”„ Table exists - clearing existing data...")
                clear_query = f"DELETE FROM {database}.{table_name}"
                clear_result = await self._execute_athena_query(athena_client, clear_query, database, output_location)
                if clear_result[0]:
                    print(f"   âœ… Existing data cleared")
                else:
                    print(f"   âš ï¸ Data clearing failed: {clear_result[1]}")
            elif create_if_not_exists:
                print(f"   ğŸ”„ Creating table: {table_name}")
                
                # ã¾ãšIcebergãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚’è©¦è¡Œï¼ˆAthena v3æ§‹æ–‡ï¼‰
                iceberg_query = f"""
                CREATE TABLE IF NOT EXISTS {database}.{table_name} (
                    stats_data_id STRING,
                    year INT,
                    region_code STRING,
                    category STRING,
                    value DOUBLE,
                    unit STRING,
                    updated_at TIMESTAMP
                )
                LOCATION 's3://{bucket}/iceberg/{table_name}/'
                TBLPROPERTIES (
                    'table_type'='ICEBERG',
                    'format'='parquet'
                )
                """
                
                iceberg_result = await self._execute_athena_query(athena_client, iceberg_query, database, output_location)
                
                if not iceberg_result[0]:
                    print(f"   âš ï¸ Iceberg creation failed: {iceberg_result[1]}")
                    print(f"   ğŸ”„ Trying regular Parquet table...")
                    
                    # é€šå¸¸ã®Parquetãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦ä½œæˆ
                    regular_query = f"""
                    CREATE EXTERNAL TABLE IF NOT EXISTS {database}.{table_name} (
                        stats_data_id STRING,
                        year INT,
                        region_code STRING,
                        category STRING,
                        value DOUBLE,
                        unit STRING,
                        updated_at TIMESTAMP
                    )
                    STORED AS PARQUET
                    LOCATION 's3://{bucket}/tables/{table_name}/'
                    """
                    
                    regular_result = await self._execute_athena_query(athena_client, regular_query, database, output_location)
                    if not regular_result[0]:
                        return {"success": False, "error": f"Failed to create table: {regular_result[1]}"}
                    else:
                        print(f"   âœ… Regular Parquet table created")
                else:
                    print(f"   âœ… Iceberg table created")
            
            # 4. å¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¦Parquetãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            external_table = f"{table_name}_temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            print(f"   ğŸ”„ Creating external table: {external_table}")
            
            # ç‰¹å®šã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹ãŸã‚ã€ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            temp_dir = f"temp/{external_table}/"
            
            # å…ƒã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            copy_source = {'Bucket': bucket, 'Key': parquet_key}
            temp_key = f"{temp_dir}{parquet_key.split('/')[-1]}"
            
            try:
                import boto3
                s3_client = boto3.client('s3', region_name=AWS_REGION)
                s3_client.copy_object(CopySource=copy_source, Bucket=bucket, Key=temp_key)
                print(f"   ğŸ“‹ Copied Parquet file to temp location: {temp_key}")
            except Exception as copy_error:
                print(f"   âš ï¸ File copy failed: {str(copy_error)}")
                # ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
                temp_dir = parquet_key.rsplit("/", 1)[0] + "/"
            
            create_external_query = f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS {database}.{external_table} (
                stats_data_id STRING,
                year INT,
                region_code STRING,
                category STRING,
                value DOUBLE,
                unit STRING,
                updated_at TIMESTAMP
            )
            STORED AS PARQUET
            LOCATION 's3://{bucket}/{temp_dir}'
            """
            
            external_result = await self._execute_athena_query(athena_client, create_external_query, database, output_location)
            if not external_result[0]:
                return {"success": False, "error": f"Failed to create external table: {external_result[1]}"}
            
            print(f"   âœ… External table created")
            
            # 5. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã«æŠ•å…¥
            print(f"   ğŸ”„ Inserting data...")
            insert_query = f"""
            INSERT INTO {database}.{table_name}
            SELECT * FROM {database}.{external_table}
            """
            
            insert_result = await self._execute_athena_query(athena_client, insert_query, database, output_location)
            if not insert_result[0]:
                # å¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
                await self._execute_athena_query(athena_client, f"DROP TABLE IF EXISTS {database}.{external_table}", database, output_location)
                return {"success": False, "error": f"Failed to insert data: {insert_result[1]}"}
            
            print(f"   âœ… Data inserted")
            
            # 6. ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’ç¢ºèª
            print(f"   ğŸ”„ Counting records...")
            count_query = f"SELECT COUNT(*) FROM {database}.{table_name}"
            count_result = await self._execute_athena_query(athena_client, count_query, database, output_location)
            
            record_count = "ä¸æ˜"
            if count_result[0] and count_result[1]:
                try:
                    if isinstance(count_result[1], list) and len(count_result[1]) > 0:
                        record_count = count_result[1][0][0] if isinstance(count_result[1][0], list) else count_result[1][0]
                    else:
                        record_count = str(count_result[1])
                except:
                    record_count = "ä¸æ˜"
            
            # 7. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            print(f"   ğŸ”„ Cleaning up external table and temp files...")
            drop_query = f"DROP TABLE IF EXISTS {database}.{external_table}"
            await self._execute_athena_query(athena_client, drop_query, database, output_location)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤
            if temp_key and temp_key != parquet_key:
                try:
                    s3_client.delete_object(Bucket=bucket, Key=temp_key)
                    print(f"   ğŸ—‘ï¸ Deleted temp file: {temp_key}")
                except Exception as delete_error:
                    print(f"   âš ï¸ Temp file deletion failed: {str(delete_error)}")
            
            print(f"   âœ… Loaded data to table: {record_count} records")
            
            return {
                "success": True,
                "table_name": table_name,
                "database": database,
                "records_loaded": record_count,
                "source_path": s3_parquet_path,
                "table_location": f"s3://{bucket}/iceberg/{table_name}/" if "iceberg" in str(iceberg_result) else f"s3://{bucket}/tables/{table_name}/",
                "message": f"Successfully loaded data to table {table_name} ({record_count} records)"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«7: analyze_with_athena
    # ========================================
    
    async def analyze_with_athena(
        self,
        table_name: str,
        analysis_type: str = "basic",
        custom_query: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Athenaã§çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
            analysis_type: åˆ†æã‚¿ã‚¤ãƒ—ï¼ˆbasic/advancedï¼‰
            custom_query: ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            åˆ†æçµæœ
        """
        print(f"\nğŸ“ˆ analyze_with_athena: {table_name} ({analysis_type})")
        
        try:
            import boto3
            
            athena_client = boto3.client('athena', region_name=AWS_REGION)
            database = 'estat_db'
            output_location = f's3://{S3_BUCKET}/athena-results/'
            
            results = {}
            
            if custom_query:
                # ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
                query_result = await self._execute_athena_query(athena_client, custom_query, database, output_location)
                results["custom_query"] = {
                    "success": query_result[0],
                    "result": query_result[1] if query_result[0] else query_result[1]
                }
            
            elif analysis_type == "basic":
                # åŸºæœ¬åˆ†æ
                queries = {
                    "record_count": f"SELECT COUNT(*) as total_records FROM {database}.{table_name}",
                    "value_stats": f"""
                        SELECT 
                            COUNT(value) as non_null_values,
                            AVG(value) as avg_value,
                            MIN(value) as min_value,
                            MAX(value) as max_value,
                            STDDEV(value) as stddev_value
                        FROM {database}.{table_name}
                        WHERE value IS NOT NULL
                    """,
                    "year_distribution": f"""
                        SELECT year, COUNT(*) as count
                        FROM {database}.{table_name}
                        GROUP BY year
                        ORDER BY year
                        LIMIT 10
                    """,
                    "category_distribution": f"""
                        SELECT category, COUNT(*) as count
                        FROM {database}.{table_name}
                        WHERE category IS NOT NULL AND category != ''
                        GROUP BY category
                        ORDER BY count DESC
                        LIMIT 10
                    """
                }
                
                for query_name, query in queries.items():
                    query_result = await self._execute_athena_query(athena_client, query, database, output_location)
                    results[query_name] = {
                        "success": query_result[0],
                        "result": query_result[1] if query_result[0] else query_result[1]
                    }
            
            elif analysis_type == "advanced":
                # é«˜åº¦ãªåˆ†æ
                queries = {
                    "correlation_analysis": f"""
                        SELECT 
                            year,
                            category,
                            AVG(value) as avg_value,
                            COUNT(*) as sample_size
                        FROM {database}.{table_name}
                        WHERE value IS NOT NULL
                        GROUP BY year, category
                        HAVING COUNT(*) >= 10
                        ORDER BY year, avg_value DESC
                        LIMIT 20
                    """,
                    "trend_analysis": f"""
                        SELECT 
                            year,
                            AVG(value) as yearly_average,
                            COUNT(*) as data_points,
                            STDDEV(value) as yearly_stddev
                        FROM {database}.{table_name}
                        WHERE value IS NOT NULL
                        GROUP BY year
                        ORDER BY year
                    """,
                    "outlier_detection": f"""
                        WITH stats AS (
                            SELECT 
                                AVG(value) as mean_val,
                                STDDEV(value) as std_val
                            FROM {database}.{table_name}
                            WHERE value IS NOT NULL
                        )
                        SELECT 
                            stats_data_id,
                            year,
                            category,
                            value,
                            ABS(value - mean_val) / std_val as z_score
                        FROM {database}.{table_name}, stats
                        WHERE value IS NOT NULL
                        AND ABS(value - mean_val) / std_val > 2
                        ORDER BY z_score DESC
                        LIMIT 10
                    """
                }
                
                for query_name, query in queries.items():
                    query_result = await self._execute_athena_query(athena_client, query, database, output_location)
                    results[query_name] = {
                        "success": query_result[0],
                        "result": query_result[1] if query_result[0] else query_result[1]
                    }
            
            # æˆåŠŸã—ãŸåˆ†æã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            successful_analyses = sum(1 for r in results.values() if r.get("success", False))
            
            print(f"   âœ… Completed {successful_analyses}/{len(results)} analyses")
            
            return {
                "success": True,
                "table_name": table_name,
                "analysis_type": analysis_type,
                "results": results,
                "successful_analyses": successful_analyses,
                "total_analyses": len(results),
                "message": f"Analysis completed: {successful_analyses}/{len(results)} queries successful"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«9: save_dataset_as_csvï¼ˆæ–°è¦ï¼‰
    # ========================================
    
    async def save_dataset_as_csv(
        self,
        dataset_id: str,
        s3_json_path: Optional[str] = None,
        local_json_path: Optional[str] = None,
        output_filename: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’CSVå½¢å¼ã§S3ã«ä¿å­˜
        
        Args:
            dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
            s3_json_path: S3ä¸Šã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            local_json_path: ãƒ­ãƒ¼ã‚«ãƒ«ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            output_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ dataset_id_YYYYMMDD_HHMMSS.csvï¼‰
        
        Returns:
            ä¿å­˜çµæœï¼ˆS3ãƒ‘ã‚¹å«ã‚€ï¼‰
        """
        print(f"\nğŸ’¾ save_dataset_as_csv: dataset_id='{dataset_id}'")
        
        try:
            import pandas as pd
            import io
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®æ±ºå®š
            data = None
            
            if s3_json_path:
                # S3ã‹ã‚‰JSONã‚’èª­ã¿è¾¼ã¿
                print(f"   ğŸ“¥ Loading from S3: {s3_json_path}")
                if not self.s3_client:
                    return {"success": False, "error": "S3 client not initialized"}
                
                # S3ãƒ‘ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
                if s3_json_path.startswith('s3://'):
                    s3_json_path = s3_json_path[5:]
                
                parts = s3_json_path.split('/', 1)
                bucket = parts[0]
                key = parts[1] if len(parts) > 1 else ''
                
                response = self.s3_client.get_object(Bucket=bucket, Key=key)
                data = json.loads(response['Body'].read().decode('utf-8'))
                
            elif local_json_path:
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
                print(f"   ğŸ“¥ Loading from local: {local_json_path}")
                with open(local_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            
            else:
                return {
                    "success": False,
                    "error": "Either s3_json_path or local_json_path must be provided"
                }
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
            value_list = stats_data.get('DATA_INF', {}).get('VALUE', [])
            
            if isinstance(value_list, dict):
                value_list = [value_list]
            
            if not value_list:
                return {"success": False, "error": "No data found in JSON"}
            
            print(f"   ğŸ“Š Converting {len(value_list):,} records to CSV")
            
            # DataFrameã«å¤‰æ›
            df = pd.DataFrame(value_list)
            
            # CSVã«å¤‰æ›
            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')  # BOMä»˜ãUTF-8
            csv_content = csv_buffer.getvalue()
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
            if not output_filename:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_filename = f"{dataset_id}_{timestamp}.csv"
            
            # S3ã«ä¿å­˜
            s3_key = f"csv/{output_filename}"
            
            if not self.s3_client:
                return {"success": False, "error": "S3 client not initialized"}
            
            self.s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=s3_key,
                Body=csv_content.encode('utf-8-sig'),
                ContentType='text/csv'
            )
            
            s3_location = f"s3://{S3_BUCKET}/{s3_key}"
            
            print(f"   âœ… CSV saved to: {s3_location}")
            
            return {
                "success": True,
                "dataset_id": dataset_id,
                "records_count": len(value_list),
                "columns": list(df.columns),
                "s3_location": s3_location,
                "s3_bucket": S3_BUCKET,
                "s3_key": s3_key,
                "filename": output_filename,
                "message": f"Successfully saved {len(value_list):,} records as CSV to S3"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ========================================
    # ãƒ„ãƒ¼ãƒ«10: download_csv_from_s3ï¼ˆæ–°è¦ï¼‰
    # ========================================
    
    async def download_csv_from_s3(
        self,
        s3_path: str,
        local_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        S3ã«ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            s3_path: S3ä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆs3://bucket/key å½¢å¼ï¼‰
            local_path: ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
        
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        print(f"\nâ¬‡ï¸  download_csv_from_s3: s3_path='{s3_path}'")
        
        try:
            if not self.s3_client:
                return {"success": False, "error": "S3 client not initialized"}
            
            # S3ãƒ‘ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            if not s3_path.startswith('s3://'):
                return {"success": False, "error": "s3_path must start with 's3://'"}
            
            s3_path_clean = s3_path[5:]
            parts = s3_path_clean.split('/', 1)
            bucket = parts[0]
            key = parts[1] if len(parts) > 1 else ''
            
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’æ±ºå®š
            if not local_path:
                filename = key.split('/')[-1]
                local_path = filename
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            import os
            local_dir = os.path.dirname(local_path)
            if local_dir and not os.path.exists(local_dir):
                os.makedirs(local_dir, exist_ok=True)
                print(f"   ğŸ“ Created directory: {local_dir}")
            
            print(f"   ğŸ“¥ Downloading from S3: {bucket}/{key}")
            print(f"   ğŸ’¾ Saving to: {local_path}")
            
            # S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆget_objectã‚’ä½¿ç”¨ã—ã¦ç›´æ¥æ›¸ãè¾¼ã¿ï¼‰
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            with open(local_path, 'wb') as f:
                f.write(response['Body'].read())
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
            import os
            file_size = os.path.getsize(local_path)
            file_size_mb = file_size / (1024 * 1024)
            
            print(f"   âœ… Downloaded: {file_size_mb:.2f} MB")
            
            return {
                "success": True,
                "s3_path": s3_path,
                "local_path": local_path,
                "file_size_bytes": file_size,
                "file_size_mb": round(file_size_mb, 2),
                "message": f"Successfully downloaded CSV to {local_path} ({file_size_mb:.2f} MB)"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    # ========================================
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # ========================================
    
    async def _execute_athena_query(self, athena_client, query: str, database: str, output_location: str):
        """Athenaã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
        try:
            import time
            
            response = athena_client.start_query_execution(
                QueryString=query,
                QueryExecutionContext={
                    'Database': database,
                    'Catalog': 'AwsDataCatalog'
                },
                ResultConfiguration={
                    'OutputLocation': output_location
                }
            )
            
            query_execution_id = response['QueryExecutionId']
            
            # ã‚¯ã‚¨ãƒªå®Œäº†ã‚’å¾…æ©Ÿ
            max_wait = 300  # 5åˆ†
            waited = 0
            
            while waited < max_wait:
                status_response = athena_client.get_query_execution(
                    QueryExecutionId=query_execution_id
                )
                status = status_response['QueryExecution']['Status']['State']
                
                if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                    break
                
                await asyncio.sleep(5)
                waited += 5
            
            if status == 'SUCCEEDED':
                # çµæœã‚’å–å¾—
                try:
                    results = athena_client.get_query_results(
                        QueryExecutionId=query_execution_id,
                        MaxResults=100
                    )
                    rows = results['ResultSet']['Rows']
                    if len(rows) > 1:
                        # æœ€åˆã®è¡Œã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿è¡Œã‚’è¿”ã™
                        data_rows = []
                        for row in rows[1:]:
                            row_data = [col.get('VarCharValue', '') for col in row['Data']]
                            data_rows.append(row_data)
                        return True, data_rows
                    else:
                        return True, "No data returned"
                except:
                    return True, "Query executed successfully"
            elif status == 'FAILED':
                error_message = status_response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                return False, error_message
            else:
                return False, f"Query timeout (status: {status})"
                
        except Exception as e:
            return False, str(e)

    def get_available_tools(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return [
            "search_and_rank_datasets",              # æ¤œç´¢ + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ + ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + Top5ï¼ˆã‚µã‚¸ã‚§ã‚¹ãƒˆæ©Ÿèƒ½ä»˜ãï¼‰
            "apply_keyword_suggestions_and_search",  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰æ›ã‚’é©ç”¨
            "fetch_dataset_auto",                    # 10ä¸‡ä»¶æœªæº€ã®è‡ªå‹•å–å¾—
            "fetch_dataset_filtered",               # ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šã§ã®çµã‚Šè¾¼ã¿å–å¾—
            "transform_to_parquet",                  # JSONã‚’Parquetã«å¤‰æ›
            "load_to_iceberg",                       # Parquetã‚’Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«æŠ•å…¥
            "analyze_with_athena"                    # Athenaã§çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ
        ]


async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("=" * 80)
    print("e-Stat HITL Analysis Server - Test Mode")
    print("=" * 80)
    
    server = EStatHITLServer()
    
    # ãƒ†ã‚¹ãƒˆ1: æ¤œç´¢ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
    print("\n" + "=" * 80)
    print("Test 1: search_and_rank_datasets (with metadata)")
    print("=" * 80)
    result = await server.search_and_rank_datasets(query="äº¤é€šäº‹æ•…", max_results=3)
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    # ãƒ†ã‚¹ãƒˆ2: ãƒ‡ãƒ¼ã‚¿å–å¾—
    if result.get('success') and result.get('results'):
        dataset_id = result['results'][0]['dataset_id']
        total_records = result['results'][0].get('total_records')
        
        print("\n" + "=" * 80)
        print("Test 2: fetch_dataset_auto")
        print("=" * 80)
        
        if total_records and total_records < LARGE_DATASET_THRESHOLD:
            fetch_result = await server.fetch_dataset_auto(dataset_id=dataset_id, save_to_s3=False)
            print(json.dumps(fetch_result, ensure_ascii=False, indent=2))
        else:
            print(f"Dataset has {total_records} records - skipping auto fetch test")


if __name__ == '__main__':
    asyncio.run(main())
