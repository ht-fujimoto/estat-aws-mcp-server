# estat-aws-remote ãƒ„ãƒ¼ãƒ«è©³ç´°è¨­è¨ˆæ›¸

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ„ãƒ¼ãƒ«1: search_estat_data](#ãƒ„ãƒ¼ãƒ«1-search_estat_data)
2. [ãƒ„ãƒ¼ãƒ«2: apply_keyword_suggestions](#ãƒ„ãƒ¼ãƒ«2-apply_keyword_suggestions)
3. [ãƒ„ãƒ¼ãƒ«3: fetch_dataset_auto](#ãƒ„ãƒ¼ãƒ«3-fetch_dataset_auto)
4. [ãƒ„ãƒ¼ãƒ«4: fetch_large_dataset_complete](#ãƒ„ãƒ¼ãƒ«4-fetch_large_dataset_complete)
5. [ãƒ„ãƒ¼ãƒ«5: fetch_dataset_filtered](#ãƒ„ãƒ¼ãƒ«5-fetch_dataset_filtered)
6. [ãƒ„ãƒ¼ãƒ«6: save_dataset_as_csv](#ãƒ„ãƒ¼ãƒ«6-save_dataset_as_csv)
7. [ãƒ„ãƒ¼ãƒ«7: save_metadata_as_csv](#ãƒ„ãƒ¼ãƒ«7-save_metadata_as_csv)
8. [ãƒ„ãƒ¼ãƒ«8: get_csv_download_url](#ãƒ„ãƒ¼ãƒ«8-get_csv_download_url)
9. [ãƒ„ãƒ¼ãƒ«9: get_estat_table_url](#ãƒ„ãƒ¼ãƒ«9-get_estat_table_url)
10. [ãƒ„ãƒ¼ãƒ«10: transform_to_parquet](#ãƒ„ãƒ¼ãƒ«10-transform_to_parquet)
11. [ãƒ„ãƒ¼ãƒ«11: load_to_iceberg](#ãƒ„ãƒ¼ãƒ«11-load_to_iceberg)
12. [ãƒ„ãƒ¼ãƒ«12: analyze_with_athena](#ãƒ„ãƒ¼ãƒ«12-analyze_with_athena)

---

## ãƒ„ãƒ¼ãƒ«1: search_estat_data

### æ¦‚è¦
è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã§e-Statçµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ã—ã€é–¢é€£æ€§ã®é«˜ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£
```python
async def search_estat_data(
    self,
    query: str,
    max_results: int = 5,
    auto_suggest: bool = True,
    scoring_method: str = "enhanced"
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| query | str | - | âœ“ | æ¤œç´¢ã‚¯ã‚¨ãƒª |
| max_results | int | 5 | - | è¿”å´ã™ã‚‹æœ€å¤§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•° |
| auto_suggest | bool | True | - | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µã‚¸ã‚§ã‚¹ãƒˆæœ‰åŠ¹åŒ– |
| scoring_method | str | "enhanced" | - | "basic" or "enhanced" |


### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 0: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µã‚¸ã‚§ã‚¹ãƒˆç¢ºèª
  â†“ (ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚ã‚Š)
Step 0a: ã‚µã‚¸ã‚§ã‚¹ãƒˆææ¡ˆã‚’è¿”å´
  â†“ (ã‚µã‚¸ã‚§ã‚¹ãƒˆãªã—/é©ç”¨å¾Œ)
Step 1: e-Stat APIå‘¼ã³å‡ºã— (getStatsList)
  â†“
Step 2: åŸºæœ¬ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° (å…¨çµæœ)
  â†“
Step 3: Top 20é¸æŠ
  â†“
Step 4: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾— (Top 20)
  â†“
Step 5: å¼·åŒ–ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
  â†“
Step 6: Top Nè¿”å´
```

### å®Ÿè£…è©³ç´°

#### Step 0: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µã‚¸ã‚§ã‚¹ãƒˆ

```python
if auto_suggest:
    keyword_suggestions = get_keyword_suggestions(query)
    if keyword_suggestions:
        suggestion_message = format_suggestion_message(keyword_suggestions)
        return {
            "success": True,
            "has_suggestions": True,
            "suggestions": {
                "original_query": query,
                "suggestions": keyword_suggestions,
                "message": suggestion_message
            }
        }
```

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ã®ä¾‹**:
- "åå…¥" â†’ "æ‰€å¾—"
- "ä¼šç¤¾" â†’ "äº‹æ¥­æ‰€"
- "å¹´é½¢" â†’ "å¹´é½¢éšç´š"

#### Step 1: e-Stat APIå‘¼ã³å‡ºã—

```python
params = {
    "appId": self.app_id,
    "searchWord": query,
    "limit": 100
}
response = await self._call_estat_api("getStatsList", params)
table_list = response.get('GET_STATS_LIST', {}).get('DATALIST_INF', {}).get('TABLE_INF', [])
```


#### Step 2: åŸºæœ¬ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

**ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¦ç´ ** (åˆè¨ˆ100%):

1. **ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒ (25%)**
```python
title = self._extract_value(dataset.get('TITLE', ''))
matches = sum(1 for keyword in query_keywords if keyword in title)
score += 0.25 * (matches / len(query_keywords))
```

2. **çµ±è¨ˆåãƒ»åˆ†é¡ãƒãƒƒãƒ (15%)**
```python
stats_name = dataset.get('STATISTICS_NAME', '')
main_cat = self._extract_value(dataset.get('MAIN_CATEGORY', ''))
sub_cat = self._extract_value(dataset.get('SUB_CATEGORY', ''))
category_text = f"{stats_name} {main_cat} {sub_cat}"
```

3. **èª¬æ˜æ–‡ãƒãƒƒãƒ (10%)**
```python
description = dataset.get('DESCRIPTION', '')
desc_matches = sum(1 for keyword in query_keywords if keyword in description)
score += 0.1 * (desc_matches / len(query_keywords))
```

4. **æ›´æ–°æ—¥ã®æ–°ã—ã• (15%)**
```python
open_date = dataset.get('OPEN_DATE', '')
date_obj = datetime.strptime(open_date, '%Y-%m-%d')
days_old = (datetime.now() - date_obj).days

if days_old <= 365:
    freshness = 1.0
elif days_old <= 1825:  # 5å¹´
    freshness = 1.0 - (days_old - 365) / 1460 * 0.5
elif days_old <= 3650:  # 10å¹´
    freshness = 0.5 - (days_old - 1825) / 1825 * 0.5
else:
    freshness = 0.0
```

5. **æ”¿åºœçµ„ç¹”ã®ä¿¡é ¼æ€§ (10%)**
```python
trusted_orgs = ['ç·å‹™çœ', 'è­¦å¯Ÿåº', 'å›½åœŸäº¤é€šçœ', 'åšç”ŸåŠ´åƒçœ', 'å†…é–£åºœ']
gov_org = self._extract_value(dataset.get('GOV_ORG', ''))
if any(org in gov_org for org in trusted_orgs):
    score += 0.1
```

6. **ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ (5%)**
```python
completeness = 0
if dataset.get('STATISTICS_NAME'): completeness += 1
if dataset.get('DESCRIPTION'): completeness += 1
if dataset.get('TITLE_SPEC'): completeness += 1
score += 0.05 * (completeness / 3)
```


#### Step 4: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å–å¾—

```python
tasks = [
    self._get_metadata_quick(item['table'].get('@id'))
    for item in top_20
]
metadata_list = await asyncio.gather(*tasks, return_exceptions=True)
```

**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å†…å®¹**:
- ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
- ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ï¼ˆåœ°åŸŸã€å¹´åº¦ãªã©ï¼‰
- 10ä¸‡ä»¶è¶…éåˆ¤å®š

#### Step 5: å¼·åŒ–ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

**è¿½åŠ è¦ç´ ** (åŸºæœ¬ã‚¹ã‚³ã‚¢80% + è¿½åŠ 20%):

7. **ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ (15%)**
```python
categories = metadata.get('categories', {})
for category_info in categories.values():
    category_values = category_info.get('values', [])
    category_text = ' '.join(category_values)
    matches = sum(1 for keyword in query_keywords if keyword in category_text)
    total_matches += matches
```

8. **ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã®é©åˆ‡æ€§ (5%)**
```python
if total_records >= 10000: return 1.0
elif total_records >= 1000: return 0.9
elif total_records >= 100: return 0.7
elif total_records >= 10: return 0.5
else: return 0.3
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼

```json
{
  "success": true,
  "query": "åŒ—æµ·é“ äººå£",
  "total_found": 150,
  "results": [
    {
      "rank": 1,
      "dataset_id": "0003458339",
      "title": "äººå£æ¨è¨ˆï¼ˆä»¤å’Œ2å¹´å›½å‹¢èª¿æŸ»åŸºæº–ï¼‰",
      "gov_org": "ç·å‹™çœ",
      "survey_date": "2020-10-01",
      "open_date": "2021-04-14",
      "score": 0.892,
      "total_records": 47000,
      "total_records_formatted": "47,000ä»¶",
      "requires_filtering": false,
      "categories": {
        "area": {
          "name": "åœ°åŸŸ",
          "count": 47,
          "sample": ["åŒ—æµ·é“", "é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ"]
        },
        "time": {
          "name": "æ™‚é–“è»¸",
          "count": 10,
          "sample": ["2020å¹´", "2021å¹´", "2022å¹´", "2023å¹´", "2024å¹´"]
        }
      }
    }
  ],
  "message": "Found 5 relevant datasets with metadata."
}
```


### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
try:
    # å‡¦ç†
except Exception as e:
    logger.error(f"Error in search_estat_data: {e}", exc_info=True)
    return format_error_response(e, "search_estat_data", {"query": query})
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

- **ä¸¦åˆ—å‡¦ç†**: Top 20ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—
- **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°**: HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒªãƒ³ã‚°
- **å®Ÿè¡Œæ™‚é–“**: é€šå¸¸2-5ç§’

---

## ãƒ„ãƒ¼ãƒ«2: apply_keyword_suggestions

### æ¦‚è¦
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰¿èªã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰æ›ã‚’é©ç”¨ã—ã¦æ–°ã—ã„ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
def apply_keyword_suggestions_tool(
    self,
    original_query: str,
    accepted_keywords: Dict[str, str]
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|------|------|
| original_query | str | âœ“ | å…ƒã®ã‚¯ã‚¨ãƒª |
| accepted_keywords | Dict[str, str] | âœ“ | æ‰¿èªã•ã‚ŒãŸå¤‰æ› {"åå…¥": "æ‰€å¾—"} |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: å…ƒã®ã‚¯ã‚¨ãƒªã‚’å˜èªåˆ†å‰²
  â†“
Step 2: å„å˜èªã‚’ãƒã‚§ãƒƒã‚¯
  â”œâ”€ å¤‰æ›å¯¾è±¡ â†’ æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç½®æ›
  â””â”€ éå¯¾è±¡ â†’ ãã®ã¾ã¾ä¿æŒ
  â†“
Step 3: æ–°ã—ã„ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
```

### å®Ÿè£…è©³ç´°

```python
def apply_keyword_suggestions_tool(self, original_query: str, accepted_keywords: Dict[str, str]):
    new_query = apply_keyword_suggestions(original_query, accepted_keywords)
    
    return {
        "success": True,
        "original_query": original_query,
        "transformed_query": new_query,
        "accepted_keywords": accepted_keywords,
        "message": f"ã‚¯ã‚¨ãƒªã‚’å¤‰æ›ã—ã¾ã—ãŸã€‚æ–°ã—ã„ã‚¯ã‚¨ãƒª: '{new_query}'"
    }
```

**keyword_dictionary.pyã®å®Ÿè£…**:
```python
def apply_keyword_suggestions(query: str, accepted_suggestions: dict) -> str:
    keywords = query.split()
    new_keywords = []
    
    for keyword in keywords:
        if keyword in accepted_suggestions:
            new_keywords.append(accepted_suggestions[keyword])
        else:
            new_keywords.append(keyword)
    
    return " ".join(new_keywords)
```

### ä½¿ç”¨ä¾‹

**å…¥åŠ›**:
```json
{
  "original_query": "å¹´é½¢åˆ¥ åå…¥",
  "accepted_keywords": {
    "å¹´é½¢åˆ¥": "å¹´é½¢éšç´š",
    "åå…¥": "æ‰€å¾—"
  }
}
```

**å‡ºåŠ›**:
```json
{
  "success": true,
  "original_query": "å¹´é½¢åˆ¥ åå…¥",
  "transformed_query": "å¹´é½¢éšç´š æ‰€å¾—",
  "accepted_keywords": {
    "å¹´é½¢åˆ¥": "å¹´é½¢éšç´š",
    "åå…¥": "æ‰€å¾—"
  },
  "message": "ã‚¯ã‚¨ãƒªã‚’å¤‰æ›ã—ã¾ã—ãŸã€‚æ–°ã—ã„ã‚¯ã‚¨ãƒª: 'å¹´é½¢éšç´š æ‰€å¾—'"
}
```


---

## ãƒ„ãƒ¼ãƒ«3: fetch_dataset_auto

### æ¦‚è¦
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’è‡ªå‹•åˆ¤å®šã—ã€æœ€é©ãªå–å¾—æ–¹æ³•ã‚’é¸æŠã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def fetch_dataset_auto(
    self,
    dataset_id: str,
    save_to_s3: bool = True,
    convert_to_japanese: bool = True
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| dataset_id | str | - | âœ“ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID |
| save_to_s3 | bool | True | - | S3ã«ä¿å­˜ã™ã‚‹ã‹ |
| convert_to_japanese | bool | True | - | ã‚³ãƒ¼ãƒ‰â†’å’Œåå¤‰æ› |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºäº‹å‰ç¢ºèª
  â”œâ”€ APIå‘¼ã³å‡ºã— (limit=1, metaGetFlg=Y)
  â””â”€ TOTAL_NUMBERå–å¾—
  â†“
Step 2: ã‚µã‚¤ã‚ºåˆ¤å®š
  â”œâ”€ â‰¤ 100,000ä»¶ â†’ _fetch_single_request()
  â””â”€ > 100,000ä»¶ â†’ fetch_large_dataset_complete()
```

### å®Ÿè£…è©³ç´°

#### Step 1: ã‚µã‚¤ã‚ºç¢ºèª

```python
test_params = {
    "appId": self.app_id,
    "statsDataId": dataset_id,
    "limit": 1,
    "metaGetFlg": "Y"
}

test_response = await self._call_estat_api("getStatsData", test_params)

total_number = test_response.get('GET_STATS_DATA', {}).get(
    'STATISTICAL_DATA', {}
).get('RESULT_INF', {}).get('TOTAL_NUMBER', 0)

logger.info(f"Dataset size: {total_number:,} records")
```

#### Step 2: è‡ªå‹•é¸æŠ

```python
LARGE_DATASET_THRESHOLD = 100000  # 10ä¸‡ä»¶

if total_number <= LARGE_DATASET_THRESHOLD:
    logger.info("Small dataset - using single request")
    return await self._fetch_single_request(dataset_id, convert_to_japanese, save_to_s3)
else:
    logger.info("Large dataset - using complete retrieval")
    return await self.fetch_large_dataset_complete(
        dataset_id=dataset_id,
        max_records=min(total_number, 1000000),
        chunk_size=100000,
        save_to_s3=save_to_s3,
        convert_to_japanese=convert_to_japanese
    )
```


### å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å–å¾— (_fetch_single_request)

```python
async def _fetch_single_request(
    self,
    dataset_id: str,
    convert_to_japanese: bool = True,
    save_to_s3: bool = True
) -> Dict[str, Any]:
    
    params = {
        "appId": self.app_id,
        "statsDataId": dataset_id,
        "limit": LARGE_DATASET_THRESHOLD
    }
    
    data = await self._call_estat_api("getStatsData", params)
    
    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
    value_list = stats_data.get('DATA_INF', {}).get('VALUE', [])
    
    # S3ä¿å­˜
    if save_to_s3 and self.s3_client:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        s3_key = f"raw/data/{dataset_id}_{timestamp}.json"
        
        self.s3_client.put_object(
            Bucket=S3_BUCKET,
            Key=s3_key,
            Body=json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'),
            ContentType='application/json'
        )
        s3_location = f"s3://{S3_BUCKET}/{s3_key}"
    
    return {
        "success": True,
        "dataset_id": dataset_id,
        "records_fetched": len(value_list),
        "expected_records": total_number,
        "completeness_ratio": 1.0,
        "s3_location": s3_location,
        "sample_data": value_list[:5]
    }
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

**å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**:
```json
{
  "success": true,
  "dataset_id": "0003458339",
  "records_fetched": 47000,
  "expected_records": 47000,
  "completeness_ratio": 1.0,
  "processing_time": "2.3ç§’",
  "sample_data": [
    {"@area": "01000", "@time": "2020", "$": "5224614"},
    {"@area": "02000", "@time": "2020", "$": "1237984"}
  ],
  "s3_location": "s3://estat-data-lake/raw/data/0003458339_20260115_143022.json",
  "message": "Successfully fetched 47,000 records (100.0% complete)"
}
```

---

## ãƒ„ãƒ¼ãƒ«4: fetch_large_dataset_complete

### æ¦‚è¦
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰²å–å¾—ã™ã‚‹ï¼ˆMCPã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶é™ã«ã‚ˆã‚Šæœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿ï¼‰ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def fetch_large_dataset_complete(
    self,
    dataset_id: str,
    max_records: int = 1000000,
    chunk_size: int = 100000,
    save_to_s3: bool = True,
    convert_to_japanese: bool = True
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| dataset_id | str | - | âœ“ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID |
| max_records | int | 1000000 | - | å–å¾—ã™ã‚‹æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° |
| chunk_size | int | 100000 | - | 1ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° |
| save_to_s3 | bool | True | - | S3ã«ä¿å­˜ã™ã‚‹ã‹ |
| convert_to_japanese | bool | True | - | ã‚³ãƒ¼ãƒ‰â†’å’Œåå¤‰æ› |


### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
  â”œâ”€ getMetaInfo API
  â””â”€ OVERALL_TOTAL_NUMBERå–å¾—
  â†“
Step 2: å®Ÿéš›ã®ç·æ•°ç¢ºèª
  â”œâ”€ getStatsData API (limit=1)
  â””â”€ TOTAL_NUMBERå–å¾—
  â†“
Step 3: å–å¾—å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°æ±ºå®š
  target_records = min(actual_total, max_records)
  â†“
Step 4: ãƒãƒ£ãƒ³ã‚¯æ•°è¨ˆç®—
  total_chunks = (target_records + chunk_size - 1) // chunk_size
  â†“
Step 5: æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯å–å¾—
  â”œâ”€ startPosition=1
  â”œâ”€ limit=chunk_size
  â””â”€ S3ä¿å­˜
  â†“
Step 6: è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿”å´
  "MCP timeout limit prevents full retrieval"
```

### å®Ÿè£…è©³ç´°

#### ãƒãƒ£ãƒ³ã‚¯å–å¾—

```python
# æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å–å¾—
params = {
    "appId": self.app_id,
    "statsDataId": dataset_id,
    "limit": chunk_size,
    "startPosition": 1
}

chunk_data = await self._call_estat_api("getStatsData", params)

chunk_values = chunk_data.get('GET_STATS_DATA', {}).get(
    'STATISTICAL_DATA', {}
).get('DATA_INF', {}).get('VALUE', [])

logger.info(f"Retrieved {len(chunk_values):,} records")
```

#### S3ä¿å­˜

```python
if save_to_s3 and self.s3_client:
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    s3_key = f"raw/data/{dataset_id}_chunk_001_{timestamp}.json"
    
    self.s3_client.put_object(
        Bucket=S3_BUCKET,
        Key=s3_key,
        Body=json.dumps(chunk_data, ensure_ascii=False, indent=2).encode('utf-8'),
        ContentType='application/json'
    )
    s3_location = f"s3://{S3_BUCKET}/{s3_key}"
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "success": true,
  "dataset_id": "0003410379",
  "metadata_total": 500000,
  "actual_total": 500000,
  "target_records": 500000,
  "chunk_size": 100000,
  "total_chunks_needed": 5,
  "chunks_retrieved": 1,
  "records_in_chunk": 100000,
  "completeness": "20.0%",
  "processing_time": "8.5ç§’",
  "sample_data": [...],
  "s3_location": "s3://estat-data-lake/raw/data/0003410379_chunk_001_20260115_143500.json",
  "next_action": "Use Python script for complete retrieval",
  "recommendation": "For complete data retrieval of 500,000 records, use the standalone Python script 'fetch_0003410379_chunked.py' to avoid MCP timeout limits",
  "message": "Retrieved first chunk (100,000 records). Total 5 chunks needed for complete dataset.",
  "warning": "MCP timeout limit prevents full retrieval. Use standalone script for complete data."
}
```

### åˆ¶é™äº‹é …

- **MCPã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: 30-60ç§’ã®åˆ¶é™ã«ã‚ˆã‚Šã€æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å–å¾—
- **å®Œå…¨å–å¾—**: ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ¨å¥¨
- **æœ€å¤§ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: 100ä¸‡ä»¶ã¾ã§


---

## ãƒ„ãƒ¼ãƒ«5: fetch_dataset_filtered

### æ¦‚è¦
ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šã§ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã‚“ã§å–å¾—ã™ã‚‹ã€‚10ä¸‡ä»¶ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«æœ‰åŠ¹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def fetch_dataset_filtered(
    self,
    dataset_id: str,
    filters: Dict[str, str],
    save_to_s3: bool = True,
    convert_to_japanese: bool = True
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| dataset_id | str | - | âœ“ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID |
| filters | Dict[str, str] | - | âœ“ | ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ |
| save_to_s3 | bool | True | - | S3ã«ä¿å­˜ã™ã‚‹ã‹ |
| convert_to_japanese | bool | True | - | ã‚³ãƒ¼ãƒ‰â†’å’Œåå¤‰æ› |

### ãƒ•ã‚£ãƒ«ã‚¿å½¢å¼

```python
filters = {
    "area": "13000",      # åœ°åŸŸã‚³ãƒ¼ãƒ‰ï¼ˆæ±äº¬éƒ½ï¼‰
    "cat01": "A1101",     # ã‚«ãƒ†ã‚´ãƒª1
    "time": "2020"        # æ™‚é–“è»¸ï¼ˆ2020å¹´ï¼‰
}
```

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
  â”œâ”€ getMetaInfo API
  â””â”€ ã‚«ãƒ†ã‚´ãƒªæƒ…å ±æŠ½å‡º
  â†“
Step 2: ãƒ•ã‚£ãƒ«ã‚¿æ¤œè¨¼ãƒ»å¤‰æ›
  â”œâ”€ æ—¥æœ¬èªå â†’ ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›
  â”œâ”€ ã‚³ãƒ¼ãƒ‰ â†’ ãã®ã¾ã¾ä½¿ç”¨
  â”œâ”€ éƒ¨åˆ†ãƒãƒƒãƒ â†’ å€™è£œææ¡ˆ
  â””â”€ ä¸æ­£å€¤ â†’ ã‚¨ãƒ©ãƒ¼è¿”å´
  â†“
Step 3: ãƒ‡ãƒ¼ã‚¿å–å¾—
  â”œâ”€ getStatsData API
  â”œâ”€ ãƒ•ã‚£ãƒ«ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ 
  â””â”€ limit=100,000
  â†“
Step 4: S3ä¿å­˜
  â†“
Step 5: çµæœè¿”å´
```

### å®Ÿè£…è©³ç´°

#### Step 1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—

```python
meta_params = {"appId": self.app_id, "statsDataId": dataset_id}
meta_data = await self._call_estat_api("getMetaInfo", meta_params)

class_objs = meta_data.get('GET_META_INFO', {}).get(
    'METADATA_INF', {}
).get('CLASS_INF', {}).get('CLASS_OBJ', [])

# ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’æŠ½å‡º
available_categories = {}
for class_obj in class_objs:
    cat_id = class_obj.get('@id')
    cat_name = class_obj.get('@name')
    classes = class_obj.get('CLASS', [])
    
    available_codes = []
    code_to_name = {}
    
    for cls in classes:
        code = cls.get('@code')
        name = cls.get('@name')
        available_codes.append(code)
        code_to_name[code] = name
    
    available_categories[cat_id] = {
        'name': cat_name,
        'codes': available_codes,
        'code_to_name': code_to_name
    }
```


#### Step 2: ãƒ•ã‚£ãƒ«ã‚¿æ¤œè¨¼ãƒ»å¤‰æ›

```python
validated_filters = {}
filter_info = {}

for filter_key, filter_value in filters.items():
    if filter_key in available_categories:
        cat_info = available_categories[filter_key]
        
        # æ—¥æœ¬èªå â†’ ã‚³ãƒ¼ãƒ‰å¤‰æ›
        if filter_value in cat_info['code_to_name'].values():
            for code, name in cat_info['code_to_name'].items():
                if name == filter_value:
                    validated_filters[f"cd{filter_key.title()}"] = code
                    filter_info[filter_key] = name
                    break
        
        # ã‚³ãƒ¼ãƒ‰ â†’ ãã®ã¾ã¾ä½¿ç”¨
        elif filter_value in cat_info['codes']:
            validated_filters[f"cd{filter_key.title()}"] = filter_value
            filter_info[filter_key] = cat_info['code_to_name'].get(filter_value)
        
        # éƒ¨åˆ†ãƒãƒƒãƒ
        else:
            partial_matches = [code for code in cat_info['codes'] if filter_value in code]
            if partial_matches:
                best_match = partial_matches[0]
                validated_filters[f"cd{filter_key.title()}"] = best_match
                filter_info[filter_key] = cat_info['code_to_name'].get(best_match)
            else:
                return {
                    "success": False,
                    "error": f"Filter value '{filter_value}' not found for category '{filter_key}'",
                    "available_codes": cat_info['codes'][:20]
                }
```

#### Step 3: ãƒ‡ãƒ¼ã‚¿å–å¾—

```python
params = {
    "appId": self.app_id,
    "statsDataId": dataset_id,
    "limit": LARGE_DATASET_THRESHOLD,
    "metaGetFlg": "Y"
}

# æ¤œè¨¼æ¸ˆã¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ 
params.update(validated_filters)

data = await self._call_estat_api("getStatsData", params)
```

### ä½¿ç”¨ä¾‹

**å…¥åŠ›**:
```json
{
  "dataset_id": "0003410379",
  "filters": {
    "area": "æ±äº¬éƒ½",
    "time": "2020"
  }
}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹**:
```json
{
  "success": true,
  "dataset_id": "0003410379",
  "filters_applied": {
    "area": "æ±äº¬éƒ½",
    "time": "2020å¹´"
  },
  "records_fetched": 5420,
  "total_available": 500000,
  "filter_effectiveness": "1.1%",
  "processing_time": "3.2ç§’",
  "sample_data": [...],
  "s3_location": "s3://estat-data-lake/raw/data/0003410379_filtered_20260115_144000.json",
  "next_action": "transform_to_parquet",
  "message": "Successfully fetched 5,420 records with filters (filtered from 500,000 total records)"
}
```

### ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹

**ä¸æ­£ãªã‚«ãƒ†ã‚´ãƒª**:
```json
{
  "success": false,
  "error": "Category 'invalid_cat' not found in dataset metadata",
  "available_categories": ["area", "cat01", "cat02", "time"],
  "suggestion": "Use one of the available category names"
}
```

**ä¸æ­£ãªå€¤**:
```json
{
  "success": false,
  "error": "Filter value '99999' not found for category 'area'",
  "available_codes": ["01000", "02000", "13000", "27000", ...],
  "suggestion": "Use one of the available codes for area"
}
```


---

## ãƒ„ãƒ¼ãƒ«6: save_dataset_as_csv

### æ¦‚è¦
å–å¾—ã—ãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã«å¤‰æ›ã—ã¦S3ã«ä¿å­˜ã™ã‚‹ã€‚Exceläº’æ›ã®BOMä»˜ãUTF-8ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def save_dataset_as_csv(
    self,
    dataset_id: str,
    s3_json_path: Optional[str] = None,
    local_json_path: Optional[str] = None,
    output_filename: Optional[str] = None
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| dataset_id | str | - | âœ“ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID |
| s3_json_path | str | None | - | S3ä¸Šã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| local_json_path | str | None | - | ãƒ­ãƒ¼ã‚«ãƒ«ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| output_filename | str | None | - | å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ±ºå®š
  â”œâ”€ s3_json_pathæŒ‡å®š â†’ S3ã‹ã‚‰èª­ã¿è¾¼ã¿
  â”œâ”€ local_json_pathæŒ‡å®š â†’ ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
  â””â”€ æœªæŒ‡å®š â†’ fetch_dataset_auto()ã§å–å¾—
  â†“
Step 2: JSONãƒ‡ãƒ¼ã‚¿æŠ½å‡º
  â”œâ”€ GET_STATS_DATA.STATISTICAL_DATA
  â””â”€ DATA_INF.VALUE
  â†“
Step 3: DataFrameå¤‰æ›
  df = pd.DataFrame(value_list)
  â†“
Step 4: CSVå¤‰æ›
  encoding='utf-8-sig' (BOMä»˜ã)
  â†“
Step 5: S3ä¿å­˜
  s3://estat-data-lake/csv/{filename}
```

### å®Ÿè£…è©³ç´°

#### Step 1: S3ã‹ã‚‰ã®èª­ã¿è¾¼ã¿

```python
if s3_json_path:
    # S3ãƒ‘ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
    if s3_json_path.startswith('s3://'):
        s3_json_path = s3_json_path[5:]
    
    parts = s3_json_path.split('/', 1)
    bucket = parts[0]
    key = parts[1]
    
    # S3ã‹ã‚‰èª­ã¿è¾¼ã¿
    response = self.s3_client.get_object(Bucket=bucket, Key=key)
    data = json.loads(response['Body'].read().decode('utf-8'))
```

#### Step 2: ãƒ‡ãƒ¼ã‚¿æŠ½å‡º

```python
stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
value_list = stats_data.get('DATA_INF', {}).get('VALUE', [])

if isinstance(value_list, dict):
    value_list = [value_list]
```

#### Step 3-4: CSVå¤‰æ›

```python
import pandas as pd
import io

df = pd.DataFrame(value_list)

csv_buffer = io.StringIO()
df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
csv_content = csv_buffer.getvalue()
```

#### Step 5: S3ä¿å­˜

```python
if not output_filename:
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_filename = f"{dataset_id}_{timestamp}.csv"

s3_key = f"csv/{output_filename}"

self.s3_client.put_object(
    Bucket=S3_BUCKET,
    Key=s3_key,
    Body=csv_content.encode('utf-8-sig'),
    ContentType='text/csv'
)

s3_location = f"s3://{S3_BUCKET}/{s3_key}"
```


### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "success": true,
  "dataset_id": "0003458339",
  "records_count": 47000,
  "columns": ["@area", "@time", "@cat01", "$", "@unit"],
  "s3_location": "s3://estat-data-lake/csv/0003458339_20260115_144500.csv",
  "s3_bucket": "estat-data-lake",
  "s3_key": "csv/0003458339_20260115_144500.csv",
  "filename": "0003458339_20260115_144500.csv",
  "message": "Successfully saved 47,000 records as CSV to S3"
}
```

### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†

S3ä¿å­˜å¤±æ•—æ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜:

```python
except Exception as s3_error:
    logger.error(f"S3 save failed, falling back to local: {s3_error}")
    local_csv_path = output_filename
    with open(local_csv_path, 'w', encoding='utf-8-sig') as f:
        f.write(csv_content)
    
    return {
        "success": True,
        "local_path": local_csv_path,
        "s3_error": str(s3_error),
        "message": "Successfully saved as CSV locally (S3 failed)"
    }
```

### ç‰¹å¾´

- **BOMä»˜ãUTF-8**: Excelã§æ–‡å­—åŒ–ã‘ã—ãªã„
- **pandasä½¿ç”¨**: é«˜é€ŸãªDataFrameå‡¦ç†
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: S3å¤±æ•—æ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
- **è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«å**: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ã

### ä¿®æ­£å±¥æ­´

**2026å¹´1æœˆ9æ—¥**: download_csv_from_s3ã®ä¿®æ­£
- `download_file`ã‹ã‚‰`get_object`ãƒ¡ã‚½ãƒƒãƒ‰ã«å¤‰æ›´
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è‡ªå‹•ä½œæˆæ©Ÿèƒ½ã‚’è¿½åŠ 
- ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ

---

## ãƒ„ãƒ¼ãƒ«7: save_metadata_as_csv

### æ¦‚è¦
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ï¼‰ã‚’CSVå½¢å¼ã§S3ã«ä¿å­˜ã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def save_metadata_as_csv(
    self,
    dataset_id: str,
    output_filename: Optional[str] = None
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| dataset_id | str | - | âœ“ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID |
| output_filename | str | None | - | å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
  getMetaInfo API
  â†“
Step 2: ã‚«ãƒ†ã‚´ãƒªæƒ…å ±æŠ½å‡º
  CLASS_INF.CLASS_OBJ
  â†“
Step 3: CSVå½¢å¼ã«å¤‰æ›
  category_id, category_name, code, name
  â†“
Step 4: S3ä¿å­˜
  s3://estat-data-lake/csv/{dataset_id}_metadata_{timestamp}.csv
```

### å®Ÿè£…è©³ç´°

#### Step 1-2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨ã‚«ãƒ†ã‚´ãƒªæŠ½å‡º

```python
# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
meta_params = {"appId": self.app_id, "statsDataId": dataset_id}
meta_data = await self._call_estat_api("getMetaInfo", meta_params)

# ã‚«ãƒ†ã‚´ãƒªæƒ…å ±æŠ½å‡º
class_objs = meta_data.get('GET_META_INFO', {}).get(
    'METADATA_INF', {}
).get('CLASS_INF', {}).get('CLASS_OBJ', [])

if not isinstance(class_objs, list):
    class_objs = [class_objs] if class_objs else []

# CSVè¡Œã‚’ç”Ÿæˆ
csv_rows = [['category_id', 'category_name', 'code', 'name']]

for class_obj in class_objs:
    cat_id = class_obj.get('@id', '')
    cat_name = class_obj.get('@name', '')
    classes = class_obj.get('CLASS', [])
    
    if not isinstance(classes, list):
        classes = [classes] if classes else []
    
    for cls in classes:
        code = cls.get('@code', '')
        name = cls.get('@name', '')
        csv_rows.append([cat_id, cat_name, code, name])
```

#### Step 3-4: CSVå¤‰æ›ã¨S3ä¿å­˜

```python
import csv
from io import StringIO

# CSVç”Ÿæˆ
csv_buffer = StringIO()
csv_writer = csv.writer(csv_buffer)
csv_writer.writerows(csv_rows)
csv_content = csv_buffer.getvalue()

# ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š
if not output_filename:
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_filename = f"{dataset_id}_metadata_{timestamp}.csv"

s3_key = f"csv/{output_filename}"

# S3ä¿å­˜
self.s3_client.put_object(
    Bucket=S3_BUCKET,
    Key=s3_key,
    Body=csv_content.encode('utf-8-sig'),
    ContentType='text/csv'
)

s3_location = f"s3://{S3_BUCKET}/{s3_key}"
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "success": true,
  "dataset_id": "0003410379",
  "categories_count": 4,
  "total_codes": 2847,
  "s3_location": "s3://estat-data-lake/csv/0003410379_metadata_20260116_150000.csv",
  "s3_bucket": "estat-data-lake",
  "s3_key": "csv/0003410379_metadata_20260116_150000.csv",
  "filename": "0003410379_metadata_20260116_150000.csv",
  "message": "Successfully saved metadata (4 categories, 2847 codes) as CSV to S3"
}
```

### CSVå‡ºåŠ›ä¾‹

```csv
category_id,category_name,code,name
tab,è¡¨ç« é …ç›®,01,äº‹æ¥­æ‰€æ•°
tab,è¡¨ç« é …ç›®,02,å¾“æ¥­è€…æ•°
cat01,ç”£æ¥­åˆ†é¡,A,è¾²æ¥­ã€æ—æ¥­
cat01,ç”£æ¥­åˆ†é¡,B,æ¼æ¥­
area,åœ°åŸŸ,00000,å…¨å›½
area,åœ°åŸŸ,01000,åŒ—æµ·é“
time,æ™‚é–“è»¸,2020,2020å¹´
time,æ™‚é–“è»¸,2021,2021å¹´
```

### ç”¨é€”

1. **ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®ç¢ºèª**: `fetch_dataset_filtered`ã§ä½¿ç”¨å¯èƒ½ãªã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
2. **ãƒ‡ãƒ¼ã‚¿ç†è§£**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã‚’æŠŠæ¡
3. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜è³‡æ–™ä½œæˆ

---

## ãƒ„ãƒ¼ãƒ«8: get_csv_download_url

### æ¦‚è¦
S3ã«ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç½²åä»˜ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’ç”Ÿæˆã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def get_csv_download_url(
    self,
    s3_path: str,
    expires_in: int = 3600,
    filename: Optional[str] = None
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| s3_path | str | - | âœ“ | S3ãƒ‘ã‚¹ (s3://bucket/key) |
| expires_in | int | 3600 | - | URLæœ‰åŠ¹æœŸé™ï¼ˆç§’ï¼‰ |
| filename | str | None | - | ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«å |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: S3ãƒ‘ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
  s3://bucket/key â†’ (bucket, key)
  â†“
Step 2: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
  head_object() â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå–å¾—
  â†“
Step 3: ç½²åä»˜ãURLç”Ÿæˆ
  generate_presigned_url()
  â†“
Step 4: URLè¿”å´
  æœ‰åŠ¹æœŸé™æƒ…å ±ä»˜ã
```

### å®Ÿè£…è©³ç´°

#### Step 1: S3ãƒ‘ã‚¹ãƒ‘ãƒ¼ã‚¹

```python
if not s3_path.startswith('s3://'):
    return {"success": False, "error": "s3_path must start with 's3://'"}

s3_path_clean = s3_path[5:]
parts = s3_path_clean.split('/', 1)
bucket = parts[0]
key = parts[1]
```

#### Step 2: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª

```python
try:
    head_response = self.s3_client.head_object(Bucket=bucket, Key=key)
    file_size = head_response.get('ContentLength', 0)
    file_size_mb = file_size / (1024 * 1024)
    logger.info(f"File size: {file_size_mb:.2f} MB")
except self.s3_client.exceptions.NoSuchKey:
    return {
        "success": False,
        "error": f"File not found in S3: {s3_path}",
        "bucket": bucket,
        "key": key
    }
```


#### Step 3: ç½²åä»˜ãURLç”Ÿæˆ

```python
# ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
if not filename:
    filename = key.split('/')[-1]

# ç½²åä»˜ãURLç”Ÿæˆ
params = {
    'Bucket': bucket,
    'Key': key,
    'ResponseContentDisposition': f'attachment; filename="{filename}"'
}

presigned_url = self.s3_client.generate_presigned_url(
    'get_object',
    Params=params,
    ExpiresIn=expires_in
)
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "success": true,
  "s3_path": "s3://estat-data-lake/csv/0003458339_20260115_144500.csv",
  "s3_bucket": "estat-data-lake",
  "s3_key": "csv/0003458339_20260115_144500.csv",
  "download_url": "https://estat-data-lake.s3.ap-northeast-1.amazonaws.com/csv/0003458339_20260115_144500.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...",
  "filename": "0003458339_20260115_144500.csv",
  "expires_in_seconds": 3600,
  "expires_at": 1736928300.0,
  "file_size_bytes": 12458960,
  "file_size_mb": 11.88,
  "processing_time_seconds": 0.15,
  "message": "ç½²åä»˜ãURLã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚ã“ã®URLã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ãã‹ã€curlã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚æœ‰åŠ¹æœŸé™: 3600ç§’"
}
```

### ä½¿ç”¨æ–¹æ³•

**ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**:
```
URLã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒãƒ¼ã«è²¼ã‚Šä»˜ã‘
```

**curlã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**:
```bash
curl -o output.csv "https://estat-data-lake.s3.ap-northeast-1.amazonaws.com/..."
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

- **æœ‰åŠ¹æœŸé™**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ™‚é–“ï¼ˆ3600ç§’ï¼‰
- **ç½²åæ¤œè¨¼**: AWSç½²åã«ã‚ˆã‚Šæ”¹ã–ã‚“é˜²æ­¢
- **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å°‚ç”¨**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚„å‰Šé™¤ã¯ä¸å¯

---

## ãƒ„ãƒ¼ãƒ«8: download_csv_from_s3

### æ¦‚è¦
S3ã«ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def download_csv_from_s3(
    self,
    s3_path: str,
    local_path: Optional[str] = None,
    return_content: bool = False
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| s3_path | str | - | âœ“ | S3ãƒ‘ã‚¹ (s3://bucket/key) |
| local_path | str | None | - | ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹ |
| return_content | bool | False | - | CSVå†…å®¹ã‚’ç›´æ¥è¿”ã™ã‹ |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: S3ãƒ‘ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
  â†“
Step 2: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹æ±ºå®š
  â”œâ”€ æŒ‡å®šã‚ã‚Š â†’ ãã®ã¾ã¾ä½¿ç”¨
  â””â”€ æŒ‡å®šãªã— â†’ ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è‡ªå‹•ç”Ÿæˆ
  â†“
Step 3: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
  os.makedirs(local_dir, exist_ok=True)
  â†“
Step 4: S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  get_object()
  â†“
Step 5: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ or å†…å®¹è¿”å´
  â”œâ”€ return_content=False â†’ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
  â””â”€ return_content=True â†’ å†…å®¹ã‚’è¿”å´
```


### å®Ÿè£…è©³ç´°

#### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹æ±ºå®š

```python
if not local_path:
    filename = key.split('/')[-1]
    local_path = filename

# ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›ï¼‰
import os
local_path = os.path.abspath(local_path)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
local_dir = os.path.dirname(local_path)
if local_dir and not os.path.exists(local_dir):
    os.makedirs(local_dir, exist_ok=True)
```

#### S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```python
# S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
try:
    head_response = self.s3_client.head_object(Bucket=bucket, Key=key)
    s3_file_size = head_response.get('ContentLength', 0)
except self.s3_client.exceptions.NoSuchKey:
    return {
        "success": False,
        "error": f"File not found in S3: {s3_path}",
        "bucket": bucket,
        "key": key
    }

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
response = self.s3_client.get_object(Bucket=bucket, Key=key)
content = response['Body'].read()
```

#### ãƒ¢ãƒ¼ãƒ‰1: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

```python
if not return_content:
    with open(local_path, 'wb') as f:
        f.write(content)
    
    file_size = os.path.getsize(local_path)
    
    # è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ
    with open(local_path, 'r', encoding='utf-8') as f:
        line_count = sum(1 for _ in f)
    
    return {
        "success": True,
        "s3_path": s3_path,
        "local_path": local_path,
        "file_size_bytes": file_size,
        "file_size_mb": round(file_size / (1024 * 1024), 2),
        "line_count": line_count,
        "message": f"Successfully downloaded CSV to {local_path}"
    }
```

#### ãƒ¢ãƒ¼ãƒ‰2: å†…å®¹è¿”å´

```python
if return_content:
    csv_content = content.decode('utf-8')
    line_count = len(csv_content.split('\n'))
    
    return {
        "success": True,
        "s3_path": s3_path,
        "content": csv_content,
        "file_size_bytes": len(content),
        "file_size_mb": round(len(content) / (1024 * 1024), 2),
        "line_count": line_count,
        "message": f"Successfully retrieved CSV content"
    }
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

**ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ¢ãƒ¼ãƒ‰**:
```json
{
  "success": true,
  "s3_path": "s3://estat-data-lake/csv/0003458339_20260115_144500.csv",
  "s3_bucket": "estat-data-lake",
  "s3_key": "csv/0003458339_20260115_144500.csv",
  "local_path": "/Users/user/Downloads/0003458339_20260115_144500.csv",
  "file_size_bytes": 12458960,
  "file_size_mb": 11.88,
  "line_count": 47001,
  "processing_time_seconds": 2.34,
  "message": "Successfully downloaded CSV to /Users/user/Downloads/0003458339_20260115_144500.csv (11.88 MB)"
}
```

**å†…å®¹è¿”å´ãƒ¢ãƒ¼ãƒ‰**:
```json
{
  "success": true,
  "s3_path": "s3://estat-data-lake/csv/0003458339_20260115_144500.csv",
  "content": "@area,@time,@cat01,$,@unit\n01000,2020,A1101,5224614,äºº\n...",
  "file_size_bytes": 12458960,
  "file_size_mb": 11.88,
  "line_count": 47001,
  "message": "Successfully retrieved CSV content (11.88 MB, 47001 lines)"
}
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ãƒ•ã‚¡ã‚¤ãƒ«æœªå­˜åœ¨**:
```json
{
  "success": false,
  "error": "File not found in S3: s3://estat-data-lake/csv/invalid.csv",
  "bucket": "estat-data-lake",
  "key": "csv/invalid.csv"
}
```

**æ¨©é™ã‚¨ãƒ©ãƒ¼**:
```json
{
  "success": false,
  "error": "Permission denied: Cannot write to /protected/path/file.csv",
  "local_path": "/protected/path/file.csv"
}
```


---

## ãƒ„ãƒ¼ãƒ«9: get_estat_table_url

### æ¦‚è¦
çµ±è¨ˆè¡¨IDã‹ã‚‰e-Statå…¬å¼ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®çµ±è¨ˆè¡¨ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
def get_estat_table_url(
    self,
    dataset_id: str
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| dataset_id | str | - | âœ“ | çµ±è¨ˆè¡¨IDï¼ˆä¾‹: 0002112323ï¼‰ |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: çµ±è¨ˆè¡¨IDã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
  â”œâ”€ ç©ºãƒã‚§ãƒƒã‚¯
  â””â”€ æ•°å­—æŠ½å‡º
  â†“
Step 2: URLç”Ÿæˆ
  https://www.e-stat.go.jp/dbview?sid={dataset_id}
  â†“
Step 3: ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿”å´
```

### å®Ÿè£…è©³ç´°

#### Step 1: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

```python
# çµ±è¨ˆè¡¨IDã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
if not dataset_id:
    return {
        "success": False,
        "error": "dataset_id is required"
    }

# çµ±è¨ˆè¡¨IDã‹ã‚‰æ•°å­—ä»¥å¤–ã‚’é™¤å»ï¼ˆå¿µã®ãŸã‚ï¼‰
clean_id = ''.join(filter(str.isdigit, str(dataset_id)))

if not clean_id:
    return {
        "success": False,
        "error": f"Invalid dataset_id: {dataset_id}. Must contain numeric characters."
    }
```

#### Step 2: URLç”Ÿæˆ

```python
# e-Statãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®URLã‚’ç”Ÿæˆ
table_url = f"https://www.e-stat.go.jp/dbview?sid={clean_id}"

logger.info(f"Generated e-Stat URL for dataset {clean_id}")
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

**æˆåŠŸæ™‚**:
```json
{
  "success": true,
  "dataset_id": "0002112323",
  "original_dataset_id": "0002112323",
  "table_url": "https://www.e-stat.go.jp/dbview?sid=0002112323",
  "processing_time_seconds": 0.0001,
  "message": "çµ±è¨ˆè¡¨ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URL: https://www.e-stat.go.jp/dbview?sid=0002112323"
}
```

**ã‚¨ãƒ©ãƒ¼æ™‚**:
```json
{
  "success": false,
  "error": "dataset_id is required"
}
```

### ä½¿ç”¨ä¾‹

**åŸºæœ¬çš„ãªä½¿ç”¨**:
```python
# çµ±è¨ˆè¡¨IDã‹ã‚‰URLç”Ÿæˆ
result = await get_estat_table_url(dataset_id="0003458339")

# ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã
print(result["table_url"])
# â†’ https://www.e-stat.go.jp/dbview?sid=0003458339
```

**æ¤œç´¢çµæœã¨çµ„ã¿åˆã‚ã›**:
```python
# 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œç´¢
search_result = await search_estat_data(query="åŒ—æµ·é“ äººå£")

# 2. æœ€ä¸Šä½ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã‚’å–å¾—
dataset_id = search_result["results"][0]["dataset_id"]

# 3. e-Statãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®URLã‚’ç”Ÿæˆ
url_result = await get_estat_table_url(dataset_id=dataset_id)

# 4. URLã‚’è¡¨ç¤º
print(f"è©³ç´°ã¯ã“ã¡ã‚‰: {url_result['table_url']}")
```

### ç‰¹å¾´

1. **é«˜é€Ÿ**: APIå‘¼ã³å‡ºã—ä¸è¦ã€å³åº§ã«URLç”Ÿæˆ
2. **ã‚·ãƒ³ãƒ—ãƒ«**: çµ±è¨ˆè¡¨IDã®ã¿ã§å‹•ä½œ
3. **å…¬å¼ãƒªãƒ³ã‚¯**: e-Statå…¬å¼ã‚µã‚¤ãƒˆã¸ã®ç›´æ¥ãƒªãƒ³ã‚¯
4. **ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: ä¸æ­£ãªIDã‚’æ¤œå‡º

### ç”¨é€”

1. **ãƒ‡ãƒ¼ã‚¿ç¢ºèª**: e-Statå…¬å¼ã‚µã‚¤ãƒˆã§çµ±è¨ˆè¡¨ã®è©³ç´°ã‚’ç¢ºèª
2. **ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ**: çµ±è¨ˆè¡¨ã¸ã®ãƒªãƒ³ã‚¯ã‚’å«ã‚€ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
3. **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ˜ç¤º**: ãƒ‡ãƒ¼ã‚¿ã®å‡ºå…¸ã‚’æ˜ç¢ºåŒ–
4. **æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: e-Statã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å ´åˆ

### e-Statçµ±è¨ˆè¡¨ãƒšãƒ¼ã‚¸ã®æƒ…å ±

ç”Ÿæˆã•ã‚ŒãŸURLã§è¡¨ç¤ºã•ã‚Œã‚‹æƒ…å ±:
- çµ±è¨ˆè¡¨ã®æ­£å¼åç§°
- èª¿æŸ»å¹´æœˆæ—¥
- å…¬é–‹æ—¥
- æä¾›çµ±è¨ˆå
- æä¾›åˆ†é¡
- è¡¨ç« é …ç›®
- åˆ†é¡äº‹é …
- ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
- Excel/CSVå½¢å¼ã§ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³

---

## ãƒ„ãƒ¼ãƒ«10: transform_to_parquet

### æ¦‚è¦
JSONãƒ‡ãƒ¼ã‚¿ã‚’Parquetå½¢å¼ã«å¤‰æ›ã—ã¦S3ã«ä¿å­˜ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’50-80%å‰Šæ¸›ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def transform_to_parquet(
    self,
    s3_json_path: str,
    data_type: str,
    output_prefix: Optional[str] = None
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| s3_json_path | str | - | âœ“ | S3ä¸Šã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| data_type | str | - | âœ“ | ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ |
| output_prefix | str | None | - | å‡ºåŠ›å…ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ |

### ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥

| data_type | ã‚¹ã‚­ãƒ¼ãƒ | ç”¨é€” |
|-----------|---------|------|
| population | year, region_code, region_name, category | äººå£çµ±è¨ˆ |
| economy | year, quarter, region_code, indicator | çµŒæ¸ˆçµ±è¨ˆ |
| education | year, region_code, school_type, metric | æ•™è‚²çµ±è¨ˆ |
| generic | year, region_code, category | æ±ç”¨ |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: S3ã‹ã‚‰JSONèª­ã¿è¾¼ã¿
  get_object()
  â†“
Step 2: ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
  GET_STATS_DATA.STATISTICAL_DATA.DATA_INF.VALUE
  â†“
Step 3: ãƒ¬ã‚³ãƒ¼ãƒ‰å¤‰æ›
  â”œâ”€ å€¤ã®æ­£è¦åŒ–ï¼ˆ'-' â†’ Noneï¼‰
  â”œâ”€ æ•°å€¤å¤‰æ›
  â””â”€ data_typeåˆ¥ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
  â†“
Step 4: DataFrameä½œæˆ
  pd.DataFrame(records)
  â†“
Step 5: Parquetå¤‰æ›
  pa.Table.from_pandas(df)
  â†“
Step 6: S3ä¿å­˜
  put_object()
```

### å®Ÿè£…è©³ç´°

#### Step 2-3: ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã¨å¤‰æ›

```python
stats_data = data.get('GET_STATS_DATA', {}).get('STATISTICAL_DATA', {})
values = stats_data.get('DATA_INF', {}).get('VALUE', [])

records = []
dataset_id = key.split('/')[-1].split('_')[0]

for value in values:
    # å€¤ã‚’å–å¾—ï¼ˆ'-'ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯Noneã«å¤‰æ›ï¼‰
    raw_value = value.get('$', '0')
    try:
        numeric_value = float(raw_value) if raw_value and raw_value != '-' else None
    except (ValueError, TypeError):
        numeric_value = None
    
    record = {
        'stats_data_id': dataset_id,
        'value': numeric_value,
        'unit': value.get('@unit', ''),
        'updated_at': datetime.now()
    }
    
    # ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã”ã¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
    if data_type == 'population':
        record['year'] = int(value.get('@time', '2020'))
        record['region_code'] = value.get('@cat01', '')
        record['region_name'] = ''
        record['category'] = value.get('@cat02', '')
    
    records.append(record)
```


#### Step 4-5: DataFrame â†’ Parquetå¤‰æ›

```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from io import BytesIO

df = pd.DataFrame(records)
table = pa.Table.from_pandas(df)

# Parquetã«å¤‰æ›
buffer = BytesIO()
pq.write_table(table, buffer)
buffer.seek(0)
```

#### Step 6: S3ä¿å­˜

```python
if output_prefix:
    parquet_key = f"{output_prefix}/{data_type}/{dataset_id}_{timestamp}.parquet"
else:
    parquet_key = key.replace('raw/data/', 'processed/').replace('.json', '.parquet')

self.s3_client.put_object(
    Bucket=bucket,
    Key=parquet_key,
    Body=buffer.getvalue(),
    ContentType='application/octet-stream'
)

s3_parquet_path = f"s3://{bucket}/{parquet_key}"
```

### ã‚¹ã‚­ãƒ¼ãƒè©³ç´°

#### population ã‚¹ã‚­ãƒ¼ãƒ

```python
{
    'stats_data_id': 'STRING',
    'year': 'INT',
    'region_code': 'STRING',
    'region_name': 'STRING',
    'category': 'STRING',
    'value': 'DOUBLE',
    'unit': 'STRING',
    'updated_at': 'TIMESTAMP'
}
```

#### economy ã‚¹ã‚­ãƒ¼ãƒ

```python
{
    'stats_data_id': 'STRING',
    'year': 'INT',
    'quarter': 'INT',
    'region_code': 'STRING',
    'indicator': 'STRING',
    'value': 'DOUBLE',
    'unit': 'STRING',
    'updated_at': 'TIMESTAMP'
}
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "success": true,
  "source_path": "s3://estat-data-lake/raw/data/0003458339_20260115_144500.json",
  "target_path": "s3://estat-data-lake/processed/0003458339_20260115_145000.parquet",
  "records_processed": 47000,
  "data_type": "population",
  "message": "Successfully converted 47000 records to Parquet format"
}
```

### Parquetã®åˆ©ç‚¹

1. **ãƒ‡ãƒ¼ã‚¿åœ§ç¸®**: 50-80%ã®ã‚µã‚¤ã‚ºå‰Šæ¸›
2. **ã‚«ãƒ©ãƒ ãƒŠãƒ¼ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: åˆ—å˜ä½ã®é«˜é€Ÿèª­ã¿è¾¼ã¿
3. **ã‚¹ã‚­ãƒ¼ãƒä¿æŒ**: ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±ã‚’ä¿æŒ
4. **é«˜é€Ÿã‚¯ã‚¨ãƒª**: Athenaã§ã®åˆ†æãŒé«˜é€Ÿ

### å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
```

---

## ãƒ„ãƒ¼ãƒ«11: load_to_iceberg

### æ¦‚è¦
Parquetãƒ‡ãƒ¼ã‚¿ã‚’Athena Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«æŠ•å…¥ã™ã‚‹ã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def load_to_iceberg(
    self,
    table_name: str,
    s3_parquet_path: str,
    create_if_not_exists: bool = True
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| table_name | str | - | âœ“ | ãƒ†ãƒ¼ãƒ–ãƒ«å |
| s3_parquet_path | str | - | âœ“ | S3ä¸Šã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| create_if_not_exists | bool | True | - | ãƒ†ãƒ¼ãƒ–ãƒ«è‡ªå‹•ä½œæˆ |


### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å­˜åœ¨ç¢ºèª/ä½œæˆ
  â”œâ”€ Glue.get_database()
  â””â”€ å­˜åœ¨ã—ãªã„å ´åˆ â†’ CREATE DATABASE
  â†“
Step 2: Icebergãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
  CREATE TABLE IF NOT EXISTS ... TBLPROPERTIES ('table_type'='ICEBERG')
  â†“
Step 3: å¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
  CREATE EXTERNAL TABLE ... STORED AS PARQUET
  â†“
Step 4: ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
  INSERT INTO {table_name} SELECT * FROM {external_table}
  â†“
Step 5: ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª
  SELECT COUNT(*) FROM {table_name}
  â†“
Step 6: å¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«å‰Šé™¤
  DROP TABLE {external_table}
```

### å®Ÿè£…è©³ç´°

#### Step 1: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ

```python
database = 'estat_db'
output_location = f's3://{S3_BUCKET}/athena-results/'

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å­˜åœ¨ç¢ºèª
try:
    import boto3
    glue_client = boto3.client('glue', region_name=AWS_REGION)
    glue_client.get_database(Name=database)
except Exception:
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
    create_db_query = f"CREATE DATABASE IF NOT EXISTS {database}"
    await self._execute_athena_query(create_db_query, database="default", output_location=output_location)
```

#### Step 2: Icebergãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ

```python
create_table_query = f"""
CREATE TABLE IF NOT EXISTS {database}.{table_name} (
    stats_data_id STRING,
    year INT,
    region_code STRING,
    category STRING,
    value DOUBLE,
    unit STRING,
    updated_at TIMESTAMP
)
LOCATION 's3://{bucket}/iceberg-tables/{table_name}/'
TBLPROPERTIES (
    'table_type'='ICEBERG',
    'format'='parquet'
)
"""

await self._execute_athena_query(create_table_query, database=database, output_location=output_location)
```

#### Step 3: å¤–éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ

```python
external_table = f"{table_name}_temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
parquet_dir = parquet_key.rsplit("/", 1)[0] + "/"

create_external_query = f"""
CREATE EXTERNAL TABLE IF NOT EXISTS {database}.{external_table} (
    stats_data_id STRING,
    year INT,
    region_code STRING,
    category STRING,
    value DOUBLE,
    unit STRING,
    updated_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://{bucket}/{parquet_dir}'
"""

await self._execute_athena_query(create_external_query, database=database, output_location=output_location)
```

#### Step 4: ãƒ‡ãƒ¼ã‚¿æŠ•å…¥

```python
insert_query = f"""
INSERT INTO {database}.{table_name}
SELECT * FROM {database}.{external_table}
"""

# Athenaãƒ¯ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
await self._execute_athena_query(insert_query, database=database, output_location=output_location)
```

**é‡è¦ãªå¤‰æ›´ç‚¹ï¼ˆ2026å¹´1æœˆ14æ—¥ï¼‰**:
- `ResultConfiguration`ã®ä»£ã‚ã‚Šã«`WorkGroup='estat-mcp-workgroup'`ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
- Athenaãƒ¯ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã§å‡ºåŠ›å ´æ‰€ã‚’ä¸€å…ƒç®¡ç†
- `s3-tables-temp-data-*`ãƒã‚±ãƒƒãƒˆã¸ã®ä¸è¦ãªå‚ç…§ã‚’å‰Šé™¤

#### Step 5: ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª

```python
count_query = f"SELECT COUNT(*) FROM {database}.{table_name}"
count_result = await self._execute_athena_query(count_query, database=database, output_location=output_location)

if count_result[0] and count_result[1]:
    record_count = count_result[1][0][0]
```


### Athenaã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼

```python
async def _execute_athena_query(
    self,
    query: str,
    database: str,
    output_location: str
) -> tuple:
    
    # S3å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    if not self.s3_client:
        return (False, "S3 client not available")
    
    try:
        self.s3_client.put_object(
            Bucket=S3_BUCKET,
            Key='athena-results/.keep',
            Body=b''
        )
        logger.info(f"Athena output location ready: {output_location}")
    except Exception as e:
        logger.error(f"Failed to create athena-results directory: {e}")
        return (False, f"Failed to setup Athena output location: {str(e)}")
    
    # ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
    try:
        response = self.athena_client.start_query_execution(
            QueryString=query,
            QueryExecutionContext={'Database': database},
            WorkGroup='estat-mcp-workgroup'
        )
    except Exception as e:
        logger.error(f"Failed to start query execution: {e}")
        return (False, f"Failed to start query: {str(e)}")
    
    query_execution_id = response['QueryExecutionId']
    
    # å®Œäº†å¾…æ©Ÿï¼ˆæœ€å¤§60ç§’ï¼‰
    max_wait = 60
    wait_interval = 2
    elapsed = 0
    
    while elapsed < max_wait:
        response = self.athena_client.get_query_execution(
            QueryExecutionId=query_execution_id
        )
        
        status = response['QueryExecution']['Status']['State']
        
        if status == 'SUCCEEDED':
            # çµæœå–å¾—
            result_response = self.athena_client.get_query_results(
                QueryExecutionId=query_execution_id
            )
            
            rows = result_response['ResultSet']['Rows']
            if len(rows) > 1:
                data_rows = []
                for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤å¤–
                    data_row = [col.get('VarCharValue', '') for col in row['Data']]
                    data_rows.append(data_row)
                return (True, data_rows)
            else:
                return (True, [])
        
        elif status in ['FAILED', 'CANCELLED']:
            error_msg = response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
            return (False, error_msg)
        
        time.sleep(wait_interval)
        elapsed += wait_interval
    
    return (False, "Query timeout")
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "success": true,
  "table_name": "population_data",
  "database": "estat_db",
  "records_loaded": "47000",
  "source_path": "s3://estat-data-lake/processed/0003458339_20260115_145000.parquet",
  "table_location": "s3://estat-data-lake/iceberg-tables/population_data/",
  "message": "Successfully loaded data to table population_data (47000 records)"
}
```

### Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã®ç‰¹å¾´

1. **ACIDä¿è¨¼**: ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œ
2. **ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«**: éå»ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‚ç…§
3. **ã‚¹ã‚­ãƒ¼ãƒé€²åŒ–**: ã‚«ãƒ©ãƒ è¿½åŠ ãƒ»å‰Šé™¤ãŒå®¹æ˜“
4. **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†**: è‡ªå‹•ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆå¤±æ•—**:
```json
{
  "success": false,
  "error": "Failed to create database: Access Denied"
}
```

**ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå¤±æ•—**:
```json
{
  "success": false,
  "error": "Failed to create Iceberg table: Invalid location"
}
```

**ãƒ‡ãƒ¼ã‚¿æŠ•å…¥å¤±æ•—**:
```json
{
  "success": false,
  "error": "Failed to insert data: Schema mismatch"
}
```

---

## ãƒ„ãƒ¼ãƒ«12: analyze_with_athena

### æ¦‚è¦
Athenaã§çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã€‚åŸºæœ¬çµ±è¨ˆã€é«˜åº¦åˆ†æã€ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªã«å¯¾å¿œã€‚

### ãƒ¡ã‚½ãƒƒãƒ‰ã‚·ã‚°ãƒãƒãƒ£

```python
async def analyze_with_athena(
    self,
    table_name: str,
    analysis_type: str = "basic",
    custom_query: Optional[str] = None
) -> Dict[str, Any]
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|-----------|------|------|
| table_name | str | - | âœ“ | ãƒ†ãƒ¼ãƒ–ãƒ«å |
| analysis_type | str | "basic" | - | åˆ†æã‚¿ã‚¤ãƒ— |
| custom_query | str | None | - | ã‚«ã‚¹ã‚¿ãƒ SQLã‚¯ã‚¨ãƒª |

### åˆ†æã‚¿ã‚¤ãƒ—

| analysis_type | å†…å®¹ |
|--------------|------|
| basic | åŸºæœ¬çµ±è¨ˆï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã€å¹³å‡ã€æœ€å°ã€æœ€å¤§ã€åˆè¨ˆã€å¹´åˆ¥é›†è¨ˆï¼‰ |
| advanced | é«˜åº¦åˆ†æï¼ˆåœ°åŸŸåˆ¥ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã€æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰ |
| custom | ã‚«ã‚¹ã‚¿ãƒ SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œ |


### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
Step 1: åˆ†æã‚¿ã‚¤ãƒ—åˆ¤å®š
  â”œâ”€ basic â†’ åŸºæœ¬çµ±è¨ˆå®Ÿè¡Œ
  â”œâ”€ advanced â†’ é«˜åº¦åˆ†æå®Ÿè¡Œ
  â””â”€ custom â†’ ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
  â†“
Step 2: SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œ
  _execute_athena_query()
  â†“
Step 3: çµæœãƒ‘ãƒ¼ã‚¹
  â†“
Step 4: çµæœè¿”å´
```

### å®Ÿè£…è©³ç´°

#### basicåˆ†æ

**1. ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**:
```python
count_query = f"SELECT COUNT(*) as total_records FROM {database}.{table_name}"
count_result = await self._execute_athena_query(count_query, database, output_location)

results["total_records"] = int(count_result[1][0][0])
```

**2. åŸºæœ¬çµ±è¨ˆ**:
```python
stats_query = f"""
SELECT 
    COUNT(*) as count,
    AVG(value) as avg_value,
    MIN(value) as min_value,
    MAX(value) as max_value,
    SUM(value) as sum_value
FROM {database}.{table_name}
WHERE value IS NOT NULL
"""

stats_result = await self._execute_athena_query(stats_query, database, output_location)

results["statistics"] = {
    "count": int(row[0]),
    "avg_value": float(row[1]),
    "min_value": float(row[2]),
    "max_value": float(row[3]),
    "sum_value": float(row[4])
}
```

**3. å¹´åˆ¥é›†è¨ˆ**:
```python
year_query = f"""
SELECT 
    year,
    COUNT(*) as count,
    AVG(value) as avg_value
FROM {database}.{table_name}
WHERE value IS NOT NULL
GROUP BY year
ORDER BY year
LIMIT 10
"""

year_result = await self._execute_athena_query(year_query, database, output_location)

results["by_year"] = [
    {
        "year": int(row[0]),
        "count": int(row[1]),
        "avg_value": float(row[2])
    }
    for row in year_result[1]
]
```

#### advancedåˆ†æ

**1. åœ°åŸŸåˆ¥é›†è¨ˆ**:
```python
region_query = f"""
SELECT 
    region_code,
    COUNT(*) as count,
    AVG(value) as avg_value,
    SUM(value) as sum_value
FROM {database}.{table_name}
WHERE value IS NOT NULL
GROUP BY region_code
ORDER BY sum_value DESC
LIMIT 10
"""

region_result = await self._execute_athena_query(region_query, database, output_location)
results["by_region"] = region_result[1]
```

**2. ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ**:
```python
category_query = f"""
SELECT 
    category,
    COUNT(*) as count,
    AVG(value) as avg_value
FROM {database}.{table_name}
WHERE value IS NOT NULL AND category IS NOT NULL
GROUP BY category
ORDER BY count DESC
LIMIT 10
"""

category_result = await self._execute_athena_query(category_query, database, output_location)
results["by_category"] = category_result[1]
```

**3. æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰**:
```python
trend_query = f"""
SELECT 
    year,
    AVG(value) as avg_value,
    MIN(value) as min_value,
    MAX(value) as max_value
FROM {database}.{table_name}
WHERE value IS NOT NULL
GROUP BY year
ORDER BY year
"""

trend_result = await self._execute_athena_query(trend_query, database, output_location)
results["trend"] = trend_result[1]
```


#### customåˆ†æ

```python
if custom_query:
    query_result = await self._execute_athena_query(custom_query, database, output_location)
    results["custom_query"] = {
        "success": query_result[0],
        "result": query_result[1] if query_result[0] else None,
        "error": query_result[1] if not query_result[0] else None
    }
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

#### basicåˆ†æ

```json
{
  "success": true,
  "table_name": "population_data",
  "database": "estat_db",
  "analysis_type": "basic",
  "results": {
    "total_records": 47000,
    "statistics": {
      "count": 47000,
      "avg_value": 2654321.5,
      "min_value": 56789,
      "max_value": 13960000,
      "sum_value": 124753310500
    },
    "by_year": [
      {"year": 2020, "count": 4700, "avg_value": 2654321.5},
      {"year": 2021, "count": 4700, "avg_value": 2648900.2},
      {"year": 2022, "count": 4700, "avg_value": 2643500.8}
    ]
  },
  "message": "Successfully analyzed table population_data"
}
```

#### advancedåˆ†æ

```json
{
  "success": true,
  "table_name": "population_data",
  "database": "estat_db",
  "analysis_type": "advanced",
  "results": {
    "by_region": [
      ["13000", "10000", "13960000.0", "139600000000.0"],
      ["27000", "10000", "8839000.0", "88390000000.0"],
      ["01000", "10000", "5224614.0", "52246140000.0"]
    ],
    "by_category": [
      ["ç·äººå£", "47000", "2654321.5"],
      ["ç”·æ€§", "47000", "1298765.2"],
      ["å¥³æ€§", "47000", "1355556.3"]
    ],
    "trend": [
      ["2020", "2654321.5", "56789.0", "13960000.0"],
      ["2021", "2648900.2", "56234.0", "13920000.0"],
      ["2022", "2643500.8", "55678.0", "13880000.0"]
    ]
  },
  "message": "Successfully analyzed table population_data"
}
```

#### customåˆ†æ

```json
{
  "success": true,
  "table_name": "population_data",
  "database": "estat_db",
  "analysis_type": "custom",
  "results": {
    "custom_query": {
      "success": true,
      "result": [
        ["åŒ—æµ·é“", "5224614"],
        ["é’æ£®çœŒ", "1237984"],
        ["å²©æ‰‹çœŒ", "1210534"]
      ],
      "error": null
    }
  },
  "message": "Successfully analyzed table population_data"
}
```

### ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªä¾‹

**åœ°åŸŸåˆ¥äººå£ãƒ©ãƒ³ã‚­ãƒ³ã‚°**:
```sql
SELECT region_code, SUM(value) as total_population
FROM estat_db.population_data
WHERE category = 'ç·äººå£'
GROUP BY region_code
ORDER BY total_population DESC
LIMIT 10
```

**å¹´åº¦åˆ¥å¢—æ¸›ç‡**:
```sql
SELECT 
    year,
    SUM(value) as total,
    LAG(SUM(value)) OVER (ORDER BY year) as prev_year,
    (SUM(value) - LAG(SUM(value)) OVER (ORDER BY year)) / LAG(SUM(value)) OVER (ORDER BY year) * 100 as growth_rate
FROM estat_db.population_data
GROUP BY year
ORDER BY year
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

- **åŸºæœ¬çµ±è¨ˆ**: 2-5ç§’
- **é«˜åº¦åˆ†æ**: 5-10ç§’
- **ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒª**: ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«ã‚ˆã‚‹

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ãƒ†ãƒ¼ãƒ–ãƒ«æœªå­˜åœ¨**:
```json
{
  "success": false,
  "error": "Table not found: estat_db.invalid_table"
}
```

**SQLã‚¨ãƒ©ãƒ¼**:
```json
{
  "success": false,
  "error": "SYNTAX_ERROR: line 1:8: Column 'invalid_column' cannot be resolved"
}
```

---

## ä»˜éŒ²: å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰

### _call_estat_api

e-Stat APIã‚’å‘¼ã³å‡ºã™ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰

```python
@retry_with_backoff(max_retries=3, base_delay=1.0)
async def _call_estat_api(self, endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:
    url = f"{self.base_url}/{endpoint}"
    
    try:
        response = self.session.get(url, params=params, timeout=60)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        raise RetryableError("e-Stat API request timed out")
    except requests.exceptions.RequestException as e:
        if response.status_code in [429, 503, 504]:
            raise RetryableError(f"e-Stat API error: {e}")
        raise EStatError(f"e-Stat API error: {e}")
```

### _get_metadata_quick

ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é«˜é€Ÿå–å¾—

```python
async def _get_metadata_quick(self, dataset_id: str) -> Dict[str, Any]:
    params = {"appId": self.app_id, "statsDataId": dataset_id}
    meta_data = await self._call_estat_api("getMetaInfo", params)
    
    # ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°è¨ˆç®—
    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±æŠ½å‡º
    # 10ä¸‡ä»¶åˆ¤å®š
    
    return {
        "total_records": total_records,
        "total_records_formatted": f"{total_records:,}ä»¶",
        "requires_filtering": total_records >= LARGE_DATASET_THRESHOLD,
        "categories": categories
    }
```

### _extract_value

ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰å€¤ã‚’æŠ½å‡º

```python
def _extract_value(self, field: Any) -> str:
    if isinstance(field, dict):
        return field.get('$', '')
    elif isinstance(field, str):
        return field
    else:
        return ''
```

---

## ã¾ã¨ã‚

### ãƒ„ãƒ¼ãƒ«åˆ†é¡

**æ¤œç´¢ç³»**:
- search_estat_data
- apply_keyword_suggestions

**å–å¾—ç³»**:
- fetch_dataset_auto
- fetch_large_dataset_complete
- fetch_dataset_filtered

**å¤‰æ›ãƒ»ä¿å­˜ç³»**:
- save_dataset_as_csv
- save_metadata_as_csv
- transform_to_parquet

**ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»URLç”Ÿæˆç³»**:
- get_csv_download_url
- get_estat_table_url

**åˆ†æç³»**:
- load_to_iceberg
- analyze_with_athena

### å…¸å‹çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: CSVå–å¾—ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
```
search_estat_data 
  â†’ fetch_dataset_auto 
  â†’ save_dataset_as_csv 
  â†’ get_csv_download_url
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèªã¨ãƒ•ã‚£ãƒ«ã‚¿å–å¾—**
```
search_estat_data 
  â†’ save_metadata_as_csv 
  â†’ get_csv_download_url (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿CSV)
  â†’ fetch_dataset_filtered (ç¢ºèªã—ãŸã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿)
  â†’ save_dataset_as_csv
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åˆ†æ**
```
search_estat_data 
  â†’ fetch_dataset_filtered 
  â†’ transform_to_parquet 
  â†’ load_to_iceberg 
  â†’ analyze_with_athena
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç¢ºèª**
```
search_estat_data 
  â†’ get_estat_table_url (e-Statå…¬å¼ã‚µã‚¤ãƒˆã§è©³ç´°ç¢ºèª)
  â†’ fetch_dataset_auto
```

---

## ä¿®æ­£å±¥æ­´

### v2.3.0 (2026å¹´1æœˆ16æ—¥)

**ãƒ„ãƒ¼ãƒ«æ§‹æˆã®ä¿®æ­£**:
1. **get_estat_table_urlè¿½åŠ **
   - çµ±è¨ˆè¡¨IDã‹ã‚‰e-Statå…¬å¼ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
   - APIå‘¼ã³å‡ºã—ä¸è¦ã®é«˜é€Ÿå‡¦ç†
   - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç¢ºèªã«ä¾¿åˆ©

2. **save_metadata_as_csvè¿½åŠ **
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚«ãƒ†ã‚´ãƒªæƒ…å ±ï¼‰ã‚’CSVä¿å­˜
   - ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®ç¢ºèªã«ä½¿ç”¨
   - ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç†è§£ã‚’æ”¯æ´

3. **ãƒ„ãƒ¼ãƒ«ç•ªå·ã®å†ç·¨æˆ**
   - å…¨12ãƒ„ãƒ¼ãƒ«ã«æ•´ç†
   - æ©Ÿèƒ½åˆ¥ã«åˆ†é¡ã‚’æ˜ç¢ºåŒ–

### v2.2.0 (2026å¹´1æœˆ16æ—¥)

**Athenaãƒ„ãƒ¼ãƒ«ã®ä¿®æ­£**:
1. **Athenaãƒ¯ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã®å°å…¥**
   - `estat-mcp-workgroup`ã‚’ä½œæˆã—ã€å‡ºåŠ›å…ˆã‚’`s3://estat-data-lake/athena-results/`ã«çµ±ä¸€
   - `ResultConfiguration`ã®ä»£ã‚ã‚Šã«`WorkGroup`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
   - `s3-tables-temp-data-*`ãƒã‚±ãƒƒãƒˆã¸ã®ä¸è¦ãªå‚ç…§ã‚’å‰Šé™¤

2. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–**
   - S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
   - Athenaå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®äº‹å‰ä½œæˆ
   - è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æä¾›

**download_csv_from_s3ã®ä¿®æ­£**:
1. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ã®å¤‰æ›´**
   - `download_file`ã‹ã‚‰`get_object`ãƒ¡ã‚½ãƒƒãƒ‰ã«å¤‰æ›´
   - ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ™‚ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ

2. **ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†ã®æ”¹å–„**
   - ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è‡ªå‹•ä½œæˆ
   - ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ä¿å­˜ã«å¯¾å¿œ

### v2.1.0 (2026å¹´1æœˆ15æ—¥)

**åˆç‰ˆãƒªãƒªãƒ¼ã‚¹**:
- å…¨11ãƒ„ãƒ¼ãƒ«ã®è©³ç´°è¨­è¨ˆã‚’å®Œæˆ
- MCP streamable-httpãƒ—ãƒ­ãƒˆã‚³ãƒ«å¯¾å¿œ
- AWS ECS Fargateãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå¯¾å¿œ

---

**ä½œæˆæ—¥**: 2026å¹´1æœˆ15æ—¥  
**æœ€çµ‚æ›´æ–°**: 2026å¹´1æœˆ16æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v2.3.0  
**ä½œæˆè€…**: Kiro AI Assistant
