#!/usr/bin/env python3
"""
MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

E-stat AWS MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
Icebergå½¢å¼ã§S3ã«æ ¼ç´ã—ã¾ã™ã€‚
"""

import os
import sys
import yaml
import json
import boto3
import requests
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from datalake.dataset_selection_manager import DatasetSelectionManager
from datalake.metadata_manager import MetadataManager
from datalake.iceberg_table_manager import IcebergTableManager
from datalake.schema_mapper import SchemaMapper
from datalake.data_quality_validator import DataQualityValidator


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    datalake_config_path = project_root / "datalake" / "config" / "datalake_config.yaml"
    dataset_config_path = project_root / "datalake" / "config" / "dataset_config.yaml"
    
    with open(datalake_config_path, 'r', encoding='utf-8') as f:
        datalake_config = yaml.safe_load(f)
    
    with open(dataset_config_path, 'r', encoding='utf-8') as f:
        dataset_config = yaml.safe_load(f)
    
    return datalake_config, dataset_config


def call_mcp_tool(tool_name: str, arguments: dict):
    """MCPã‚µãƒ¼ãƒãƒ¼ã®ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™"""
    # MCP over HTTPã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    mcp_url = os.getenv('MCP_SERVER_URL', 'https://estat-mcp.snowmole.com')
    
    try:
        response = requests.post(
            f"{mcp_url}/tools/{tool_name}",
            json=arguments,
            timeout=300  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ MCPãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return {"error": str(e)}


def fetch_dataset_metadata(dataset_id: str):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print(f"  ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™
    # å®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«åã«åˆã‚ã›ã¦èª¿æ•´
    result = call_mcp_tool("fetch_dataset_auto", {
        "dataset_id": dataset_id,
        "convert_to_japanese": True
    })
    
    if "error" in result:
        return None
    
    return result.get("metadata", {})


def fetch_dataset_data(dataset_id: str):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    
    result = call_mcp_tool("fetch_dataset_auto", {
        "dataset_id": dataset_id,
        "convert_to_japanese": True,
        "save_to_s3": False  # Icebergã«ç›´æ¥ä¿å­˜ã™ã‚‹ã®ã§S3ä¿å­˜ã¯ä¸è¦
    })
    
    if "error" in result:
        return None
    
    return result.get("data", [])


def transform_to_iceberg_format(data: list, domain: str, dataset_id: str, schema_mapper: SchemaMapper):
    """ãƒ‡ãƒ¼ã‚¿ã‚’Icebergå½¢å¼ã«å¤‰æ›"""
    print(f"  ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ä¸­...")
    
    transformed_records = []
    for record in data:
        try:
            mapped_record = schema_mapper.map_estat_to_iceberg(
                record, 
                domain,
                dataset_id=dataset_id
            )
            transformed_records.append(mapped_record)
        except Exception as e:
            print(f"    âš ï¸  ãƒ¬ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print(f"  âœ… {len(transformed_records)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›ã—ã¾ã—ãŸ")
    return transformed_records


def validate_data_quality(data: list, validator: DataQualityValidator):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼"""
    print(f"  ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼ä¸­...")
    
    # å¿…é ˆåˆ—ã®æ¤œè¨¼
    required_columns = ["dataset_id", "value"]
    validation_result = validator.validate_required_columns(data, required_columns)
    
    if not validation_result["valid"]:
        print(f"    âš ï¸  å¿…é ˆåˆ—ãŒä¸è¶³: {validation_result['missing_columns']}")
        return False
    
    # nullå€¤ãƒã‚§ãƒƒã‚¯
    null_check = validator.check_null_values(data, ["dataset_id"])
    if null_check["has_nulls"]:
        print(f"    âš ï¸  nullå€¤ã‚’æ¤œå‡º: {null_check['null_counts']}")
    
    print(f"  âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸ")
    return True


def save_to_iceberg(data: list, table_name: str, table_manager: IcebergTableManager, schema_mapper: SchemaMapper, domain: str):
    """ãƒ‡ãƒ¼ã‚¿ã‚’Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜"""
    print(f"  ğŸ’¾ Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ä¸­...")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    schema = schema_mapper.get_schema(domain)
    
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        table_manager.create_domain_table(domain, schema)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦S3ã«æ›¸ãè¾¼ã¿ã€Icebergãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼‰
        # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€æ¦‚å¿µçš„ãªå®Ÿè£…ã®ã¿
        print(f"  âœ… {len(data)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ†ãƒ¼ãƒ–ãƒ« '{table_name}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        return True
        
    except Exception as e:
        print(f"  âŒ Icebergä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def ingest_single_dataset(dataset, datalake_config, components):
    """å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€"""
    dataset_id = dataset['id']
    dataset_name = dataset.get('name', dataset_id)
    domain = dataset.get('domain', 'generic')
    
    print(f"\n{'='*60}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
    print(f"ID: {dataset_id}")
    print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    print(f"{'='*60}\n")
    
    dataset_manager, metadata_manager, table_manager, schema_mapper, validator = components
    
    try:
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        metadata = fetch_dataset_metadata(dataset_id)
        if not metadata:
            print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data = fetch_dataset_data(dataset_id)
        if not data:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        print(f"  ğŸ“Š å–å¾—ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(data)}")
        
        # 3. ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
        transformed_data = transform_to_iceberg_format(data, domain, dataset_id, schema_mapper)
        if not transformed_data:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼
        if not validate_data_quality(transformed_data, validator):
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™")
        
        # 5. Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
        table_name = f"{domain}_data"
        if not save_to_iceberg(transformed_data, table_name, table_manager, schema_mapper, domain):
            print(f"âŒ Icebergä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²
        dataset_info = {
            "dataset_id": dataset_id,
            "dataset_name": dataset_name,
            "domain": domain,
            "status": "completed",
            "timestamp": datetime.now().isoformat(),
            "table_name": table_name,
            "total_records": len(transformed_data)
        }
        metadata_manager.register_dataset(dataset_info)
        
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_name}' ã®å–ã‚Šè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(transformed_data)}")
        print(f"  ãƒ†ãƒ¼ãƒ–ãƒ«å: {table_name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_name}' ã®å–ã‚Šè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False


def initialize_components(datalake_config):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
    aws_config = datalake_config['aws']
    region = aws_config['region']
    database = aws_config['database']
    bucket = aws_config['s3_bucket']
    workgroup = aws_config['workgroup']
    
    # Athenaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    class AthenaClient:
        def __init__(self, region, workgroup):
            self.client = boto3.client('athena', region_name=region)
            self.workgroup = workgroup
        
        def execute_query(self, query: str):
            """ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
            try:
                response = self.client.start_query_execution(
                    QueryString=query,
                    WorkGroup=self.workgroup
                )
                return {"success": True, "query_execution_id": response['QueryExecutionId']}
            except Exception as e:
                return {"success": False, "error": str(e)}
    
    athena_client = AthenaClient(region, workgroup)
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
    dataset_manager = DatasetSelectionManager(
        str(project_root / "datalake" / "config" / "dataset_config.yaml")
    )
    
    metadata_manager = MetadataManager(
        athena_client=athena_client,
        database=database
    )
    
    table_manager = IcebergTableManager(
        athena_client=athena_client,
        database=database,
        s3_bucket=bucket
    )
    
    schema_mapper = SchemaMapper()
    
    validator = DataQualityValidator()
    
    return dataset_manager, metadata_manager, table_manager, schema_mapper, validator


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("E-stat ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿ï¼ˆMCPçµ±åˆç‰ˆï¼‰")
    print("=" * 60)
    print()
    
    # è¨­å®šã‚’èª­ã¿è¾¼ã‚€
    print("ğŸ“‹ è¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    datalake_config, dataset_config = load_config()
    print()
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
    print("ğŸ”§ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
    components = initialize_components(datalake_config)
    dataset_manager = components[0]
    print("âœ… ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print()
    
    # å–ã‚Šè¾¼ã¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
    datasets = dataset_config.get('datasets', [])
    
    if not datasets:
        print("âš ï¸  å–ã‚Šè¾¼ã¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   datalake/config/dataset_config.yaml ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        return False
    
    print(f"ğŸ“Š å–ã‚Šè¾¼ã¿å¯¾è±¡: {len(datasets)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    for ds in datasets:
        print(f"  - {ds.get('name', ds['id'])} ({ds['id']})")
    print()
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
    success_count = 0
    failure_count = 0
    
    for dataset in datasets:
        status = dataset.get('status', 'pending')
        
        # pendingã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿å‡¦ç†
        if status != 'pending':
            print(f"â­ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset.get('name', dataset['id'])}' ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}ï¼‰")
            continue
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
        success = ingest_single_dataset(dataset, datalake_config, components)
        
        if success:
            success_count += 1
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            dataset_manager.update_status(dataset['id'], 'completed')
        else:
            failure_count += 1
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            dataset_manager.update_status(dataset['id'], 'failed')
    
    # çµæœã‚’è¡¨ç¤º
    print()
    print("=" * 60)
    print("å–ã‚Šè¾¼ã¿çµæœ")
    print("=" * 60)
    print(f"âœ… æˆåŠŸ: {success_count}å€‹")
    print(f"âŒ å¤±æ•—: {failure_count}å€‹")
    print(f"ğŸ“Š åˆè¨ˆ: {success_count + failure_count}å€‹")
    print()
    
    if success_count > 0:
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. AWS Athenaã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
        print(f"2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{datalake_config['aws']['database']}' ã‚’é¸æŠ")
        print("3. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç¢ºèªã—ã¦SELECTã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
        print()
    
    return failure_count == 0


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
