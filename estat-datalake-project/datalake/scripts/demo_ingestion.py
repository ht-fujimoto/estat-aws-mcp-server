#!/usr/bin/env python3
"""
ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿéš›ã®E-statãƒ‡ãƒ¼ã‚¿ã‚’MCPãƒ„ãƒ¼ãƒ«ã§å–å¾—ã—ã€
å‡¦ç†ã®æµã‚Œã‚’å®Ÿæ¼”ã—ã¾ã™ã€‚
"""

import sys
import json
import boto3
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from datalake.schema_mapper import SchemaMapper
from datalake.data_quality_validator import DataQualityValidator


def fetch_data_via_mcp(dataset_id: str):
    """MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    print(f"  ğŸ“Š MCPãƒ„ãƒ¼ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    print(f"     dataset_id: {dataset_id}")
    
    # å®Ÿéš›ã®MCPãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    # å®Ÿç’°å¢ƒã§ã¯ mcp_estat_aws_remote_fetch_dataset_auto ã‚’å‘¼ã³å‡ºã™
    
    sample_data = [
        {
            "@tab": "014",
            "@cat01": "001",  # ç”·å¥³è¨ˆ
            "@cat02": "001",  # ç·äººå£
            "@cat03": "01000",  # ç·æ•°
            "@area": "00000",  # å…¨å›½
            "@time": "2020",
            "@unit": "åƒäºº",
            "$": "126146"
        },
        {
            "@tab": "014",
            "@cat01": "002",  # ç”·
            "@cat02": "001",  # ç·äººå£
            "@cat03": "01000",  # ç·æ•°
            "@area": "00000",  # å…¨å›½
            "@time": "2020",
            "@unit": "åƒäºº",
            "$": "61350"
        },
        {
            "@tab": "014",
            "@cat01": "003",  # å¥³
            "@cat02": "001",  # ç·äººå£
            "@cat03": "01000",  # ç·æ•°
            "@area": "00000",  # å…¨å›½
            "@time": "2020",
            "@unit": "åƒäºº",
            "$": "64796"
        }
    ]
    
    print(f"  âœ… {len(sample_data)}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
    print(f"     ï¼ˆå®Ÿéš›ã¯4,080ä»¶ï¼‰")
    return sample_data


def transform_data(data: list, domain: str, dataset_id: str = None):
    """ãƒ‡ãƒ¼ã‚¿ã‚’Icebergå½¢å¼ã«å¤‰æ›"""
    print(f"\n  ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ä¸­...")
    print(f"     ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    
    schema_mapper = SchemaMapper()
    transformed_data = []
    
    for record in data:
        try:
            mapped_record = schema_mapper.map_estat_to_iceberg(
                record, 
                domain,
                dataset_id=dataset_id
            )
            transformed_data.append(mapped_record)
        except Exception as e:
            print(f"     âš ï¸  ãƒ¬ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print(f"  âœ… {len(transformed_data)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›ã—ã¾ã—ãŸ")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
    if transformed_data:
        print(f"\n  ğŸ“‹ å¤‰æ›å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰:")
        sample = transformed_data[0]
        for key, value in list(sample.items())[:5]:
            print(f"     {key}: {value}")
        if len(sample) > 5:
            print(f"     ... (ä»– {len(sample) - 5} ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰)")
    
    return transformed_data


def validate_data(data: list):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼"""
    print(f"\n  ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼ä¸­...")
    
    validator = DataQualityValidator()
    
    # å¿…é ˆåˆ—ã®æ¤œè¨¼
    required_columns = ["dataset_id", "value"]
    validation_result = validator.validate_required_columns(data, required_columns)
    
    if validation_result["valid"]:
        print(f"  âœ… å¿…é ˆåˆ—ã®æ¤œè¨¼: åˆæ ¼")
    else:
        print(f"  âš ï¸  å¿…é ˆåˆ—ãŒä¸è¶³: {validation_result['missing_columns']}")
    
    # nullå€¤ãƒã‚§ãƒƒã‚¯
    null_check = validator.check_null_values(data, ["dataset_id"])
    if null_check["has_nulls"]:
        print(f"  âš ï¸  nullå€¤ã‚’æ¤œå‡º: {null_check['null_counts']}")
    else:
        print(f"  âœ… nullå€¤ãƒã‚§ãƒƒã‚¯: åˆæ ¼")
    
    return validation_result["valid"]


def save_to_iceberg_simulation(data: list, table_name: str):
    """Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ä¿å­˜ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    print(f"\n  ğŸ’¾ Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ä¸­...")
    print(f"     ãƒ†ãƒ¼ãƒ–ãƒ«å: {table_name}")
    print(f"     ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(data)}")
    
    # å®Ÿéš›ã®ä¿å­˜å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print(f"\n  ğŸ“ ä¿å­˜æ‰‹é †:")
    print(f"     1. Parquetå½¢å¼ã«å¤‰æ›")
    print(f"     2. S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (s3://estat-iceberg-datalake/iceberg-tables/population/)")
    print(f"     3. Icebergãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°")
    print(f"     4. Glue Catalogã«ç™»éŒ²")
    
    print(f"\n  âœ… ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
    
    return True


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("E-stat ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ãƒ‡ãƒ¢")
    print("=" * 60)
    print()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset_id = "0004021107"
    dataset_name = "å¹´é½¢ï¼ˆå„æ­³ï¼‰ï¼Œç”·å¥³åˆ¥äººå£åŠã³äººå£æ€§æ¯”"
    domain = "population"
    table_name = f"{domain}_data"
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:")
    print(f"  ID: {dataset_id}")
    print(f"  åå‰: {dataset_name}")
    print(f"  ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    print(f"  ãƒ†ãƒ¼ãƒ–ãƒ«å: {table_name}")
    print()
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å–å¾—
        print("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å–å¾—")
        print("-" * 60)
        data = fetch_data_via_mcp(dataset_id)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        print("ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›")
        print("-" * 60)
        transformed_data = transform_data(data, domain, dataset_id=dataset_id)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
        print("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼")
        print("-" * 60)
        is_valid = validate_data(transformed_data)
        print()
        
        if not is_valid:
            print("âš ï¸  ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™")
            print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: Icebergä¿å­˜
        print("ã‚¹ãƒ†ãƒƒãƒ—4: Icebergä¿å­˜")
        print("-" * 60)
        save_to_iceberg_simulation(transformed_data, table_name)
        print()
        
        # å®Œäº†
        print("=" * 60)
        print("âœ… ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 60)
        print()
        
        print("å®Ÿéš›ã®å–ã‚Šè¾¼ã¿ã§ã¯:")
        print("  - 4,080ä»¶ã®å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‡¦ç†")
        print("  - S3ã«Parquetå½¢å¼ã§ä¿å­˜")
        print("  - Glue Catalogã«ç™»éŒ²")
        print("  - Athenaã§ã‚¯ã‚¨ãƒªå¯èƒ½")
        print()
        
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. AWS Athenaã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚’é–‹ã")
        print("  2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ 'estat_iceberg_db' ã‚’é¸æŠ")
        print("  3. ãƒ†ãƒ¼ãƒ–ãƒ« 'population_data' ã‚’ã‚¯ã‚¨ãƒª")
        print()
        
        print("ã‚¯ã‚¨ãƒªä¾‹:")
        print("  SELECT * FROM population_data LIMIT 10;")
        print("  SELECT year, SUM(value) as total FROM population_data GROUP BY year;")
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
