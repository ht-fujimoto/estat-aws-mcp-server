#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

dataset_config.yamlã«å®šç¾©ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’E-stat APIã‹ã‚‰å–å¾—ã—ã€
Icebergå½¢å¼ã§S3ã«æ ¼ç´ã—ã¾ã™ã€‚
"""

import os
import sys
import yaml
import json
import asyncio
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from datalake.dataset_selection_manager import DatasetSelectionManager
from datalake.data_ingestion_orchestrator import DataIngestionOrchestrator
from datalake.metadata_manager import MetadataManager
from datalake.iceberg_table_manager import IcebergTableManager
from datalake.schema_mapper import SchemaMapper
from datalake.error_handler import ErrorHandler


def load_config():
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    datalake_config_path = project_root / "datalake" / "config" / "datalake_config.yaml"
    dataset_config_path = project_root / "datalake" / "config" / "dataset_config.yaml"
    
    with open(datalake_config_path, 'r', encoding='utf-8') as f:
        datalake_config = yaml.safe_load(f)
    
    with open(dataset_config_path, 'r', encoding='utf-8') as f:
        dataset_config = yaml.safe_load(f)
    
    return datalake_config, dataset_config


def initialize_components(datalake_config):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
    import boto3
    
    aws_config = datalake_config['aws']
    region = aws_config['region']
    database = aws_config['database']
    bucket = aws_config['s3_bucket']
    workgroup = aws_config['workgroup']
    
    # Athenaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    class AthenaClient:
        def __init__(self, region, workgroup):
            self.client = boto3.client('athena', region_name=region)
            self.workgroup = workgroup
        
        def execute_query(self, query: str):
            """ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
            try:
                response = self.client.start_query_execution(
                    QueryString=query,
                    WorkGroup=self.workgroup
                )
                return {"success": True, "query_execution_id": response['QueryExecutionId']}
            except Exception as e:
                return {"success": False, "error": str(e)}
    
    athena_client = AthenaClient(region, workgroup)
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
    dataset_manager = DatasetSelectionManager(
        str(project_root / "datalake" / "config" / "dataset_config.yaml")
    )
    
    metadata_manager = MetadataManager(
        athena_client=athena_client,
        database=database
    )
    
    table_manager = IcebergTableManager(
        athena_client=athena_client,
        database=database,
        s3_bucket=bucket
    )
    
    schema_mapper = SchemaMapper()
    
    error_handler = ErrorHandler(
        max_retries=3,
        base_delay=2.0,
        max_delay=60.0
    )
    
    # MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ç½®ãæ›ãˆï¼‰
    class MockMCPClient:
        """ãƒ¢ãƒƒã‚¯MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
        def call_tool(self, tool_name: str, arguments: dict):
            print(f"  [MCP] {tool_name}({json.dumps(arguments, ensure_ascii=False)})")
            # å®Ÿéš›ã®MCPã‚µãƒ¼ãƒãƒ¼ã‚’å‘¼ã³å‡ºã™å®Ÿè£…ã«ç½®ãæ›ãˆã‚‹
            return {"success": True, "data": []}
    
    mcp_client = MockMCPClient()
    
    orchestrator = DataIngestionOrchestrator(
        mcp_client=mcp_client,
        metadata_manager=metadata_manager,
        table_manager=table_manager,
        schema_mapper=schema_mapper,
        error_handler=error_handler
    )
    
    return dataset_manager, orchestrator, metadata_manager


async def ingest_single_dataset(orchestrator, dataset):
    """å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€"""
    dataset_id = dataset['id']
    dataset_name = dataset.get('name', dataset_id)
    domain = dataset.get('domain', 'generic')
    
    print(f"\n{'='*60}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
    print(f"ID: {dataset_id}")
    print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    print(f"{'='*60}\n")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
        result = await orchestrator.ingest_dataset(
            dataset_id=dataset_id,
            domain=domain
        )
        
        if result.get("success"):
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_name}' ã®å–ã‚Šè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")
            print(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {result.get('record_count', 0)}")
            print(f"  ãƒ†ãƒ¼ãƒ–ãƒ«å: {result.get('table_name', 'N/A')}")
            return True
        else:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_name}' ã®å–ã‚Šè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print(f"  ã‚¨ãƒ©ãƒ¼: {result.get('error', 'Unknown')}")
            return False
            
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_name}' ã®å–ã‚Šè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("E-stat ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿")
    print("=" * 60)
    print()
    
    # è¨­å®šã‚’èª­ã¿è¾¼ã‚€
    print("ğŸ“‹ è¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    datalake_config, dataset_config = load_config()
    print()
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
    print("ğŸ”§ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
    dataset_manager, orchestrator, metadata_manager = initialize_components(datalake_config)
    print("âœ… ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print()
    
    # å–ã‚Šè¾¼ã¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
    datasets = dataset_config.get('datasets', [])
    
    if not datasets:
        print("âš ï¸  å–ã‚Šè¾¼ã¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   datalake/config/dataset_config.yaml ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        return False
    
    print(f"ğŸ“Š å–ã‚Šè¾¼ã¿å¯¾è±¡: {len(datasets)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    for ds in datasets:
        print(f"  - {ds.get('name', ds['id'])} ({ds['id']})")
    print()
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
    success_count = 0
    failure_count = 0
    
    for dataset in datasets:
        status = dataset.get('status', 'pending')
        
        # pendingã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿å‡¦ç†
        if status != 'pending':
            print(f"â­ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset.get('name', dataset['id'])}' ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}ï¼‰")
            continue
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã‚€
        success = await ingest_single_dataset(orchestrator, dataset)
        
        if success:
            success_count += 1
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            dataset_manager.update_status(dataset['id'], 'completed')
        else:
            failure_count += 1
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            dataset_manager.update_status(dataset['id'], 'failed')
    
    # çµæœã‚’è¡¨ç¤º
    print()
    print("=" * 60)
    print("å–ã‚Šè¾¼ã¿çµæœ")
    print("=" * 60)
    print(f"âœ… æˆåŠŸ: {success_count}å€‹")
    print(f"âŒ å¤±æ•—: {failure_count}å€‹")
    print(f"ğŸ“Š åˆè¨ˆ: {success_count + failure_count}å€‹")
    print()
    
    if success_count > 0:
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. AWS Athenaã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
        print(f"2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ '{datalake_config['aws']['database']}' ã‚’é¸æŠ")
        print("3. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç¢ºèªã—ã¦SELECTã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
        print()
    
    return failure_count == 0


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
