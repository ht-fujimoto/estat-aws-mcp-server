#!/usr/bin/env python3
"""
å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

MCPãƒ„ãƒ¼ãƒ«ã§å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’Icebergå½¢å¼ã«å¤‰æ›ã—ã¦S3ã«ä¿å­˜ã—ã¾ã™ã€‚
"""

import sys
import json
import boto3
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from datalake.schema_mapper import SchemaMapper
from datalake.data_quality_validator import DataQualityValidator


def load_data_from_s3(s3_path: str):
    """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print(f"  ğŸ“¥ S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    print(f"     ãƒ‘ã‚¹: {s3_path}")
    
    # S3ãƒ‘ã‚¹ã‚’è§£æ
    if s3_path.startswith("s3://"):
        s3_path = s3_path[5:]
    
    parts = s3_path.split("/", 1)
    bucket = parts[0]
    key = parts[1]
    
    # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    s3 = boto3.client('s3')
    
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        data = json.loads(response['Body'].read().decode('utf-8'))
        
        print(f"  âœ… {len(data)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return data
    except Exception as e:
        print(f"  âŒ S3èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def transform_data(data: list, domain: str, dataset_id: str):
    """ãƒ‡ãƒ¼ã‚¿ã‚’Icebergå½¢å¼ã«å¤‰æ›"""
    print(f"\n  ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ä¸­...")
    print(f"     ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    print(f"     ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID: {dataset_id}")
    
    schema_mapper = SchemaMapper()
    transformed_data = []
    
    for record in data:
        try:
            mapped_record = schema_mapper.map_estat_to_iceberg(
                record, 
                domain,
                dataset_id=dataset_id
            )
            transformed_data.append(mapped_record)
        except Exception as e:
            print(f"     âš ï¸  ãƒ¬ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print(f"  âœ… {len(transformed_data)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›ã—ã¾ã—ãŸ")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
    if transformed_data:
        print(f"\n  ğŸ“‹ å¤‰æ›å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰:")
        sample = transformed_data[0]
        for key, value in list(sample.items())[:8]:
            if key == "updated_at":
                print(f"     {key}: {value.isoformat()}")
            else:
                print(f"     {key}: {value}")
    
    return transformed_data


def validate_data(data: list):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼"""
    print(f"\n  ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’æ¤œè¨¼ä¸­...")
    
    validator = DataQualityValidator()
    
    # å¿…é ˆåˆ—ã®æ¤œè¨¼
    required_columns = ["dataset_id", "value"]
    validation_result = validator.validate_required_columns(data, required_columns)
    
    if validation_result["valid"]:
        print(f"  âœ… å¿…é ˆåˆ—ã®æ¤œè¨¼: åˆæ ¼")
    else:
        print(f"  âš ï¸  å¿…é ˆåˆ—ãŒä¸è¶³: {validation_result['missing_columns']}")
        return False
    
    # nullå€¤ãƒã‚§ãƒƒã‚¯
    null_check = validator.check_null_values(data, ["dataset_id"])
    if null_check["has_nulls"]:
        print(f"  âš ï¸  nullå€¤ã‚’æ¤œå‡º: {null_check['null_counts']}")
        return False
    else:
        print(f"  âœ… nullå€¤ãƒã‚§ãƒƒã‚¯: åˆæ ¼")
    
    return True


def save_to_parquet(data: list, output_path: str):
    """Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦S3ã«ä¿å­˜"""
    print(f"\n  ğŸ’¾ Parquetå½¢å¼ã§ä¿å­˜ä¸­...")
    print(f"     å‡ºåŠ›ãƒ‘ã‚¹: {output_path}")
    
    try:
        import pandas as pd
        import pyarrow as pa
        import pyarrow.parquet as pq
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(data)
        
        # S3ãƒ‘ã‚¹ã‚’è§£æ
        if output_path.startswith("s3://"):
            output_path = output_path[5:]
        
        parts = output_path.split("/", 1)
        bucket = parts[0]
        key = parts[1]
        
        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        table = pa.Table.from_pandas(df)
        
        # S3ã«æ›¸ãè¾¼ã¿
        s3 = boto3.client('s3')
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as tmp:
            pq.write_table(table, tmp.name)
            tmp_path = tmp.name
        
        # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        s3.upload_file(tmp_path, bucket, key)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        import os
        os.unlink(tmp_path)
        
        print(f"  âœ… {len(data)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        return True
        
    except Exception as e:
        print(f"  âŒ Parquetä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("E-stat å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿")
    print("=" * 60)
    print()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset_id = "0004021107"
    dataset_name = "å¹´é½¢ï¼ˆå„æ­³ï¼‰ï¼Œç”·å¥³åˆ¥äººå£åŠã³äººå£æ€§æ¯”"
    domain = "population"
    s3_input_path = "s3://estat-data-lake/raw/data/0004021107_20260119_052606.json"
    s3_output_path = f"s3://estat-iceberg-datalake/parquet/{domain}/{dataset_id}.parquet"
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:")
    print(f"  ID: {dataset_id}")
    print(f"  åå‰: {dataset_name}")
    print(f"  ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    print(f"  å…¥åŠ›: {s3_input_path}")
    print(f"  å‡ºåŠ›: {s3_output_path}")
    print()
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        print("ã‚¹ãƒ†ãƒƒãƒ—1: S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        print("-" * 60)
        data = load_data_from_s3(s3_input_path)
        if not data:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        print("ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›")
        print("-" * 60)
        transformed_data = transform_data(data, domain, dataset_id=dataset_id)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
        print("ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼")
        print("-" * 60)
        is_valid = validate_data(transformed_data)
        print()
        
        if not is_valid:
            print("âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            return False
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: Parquetä¿å­˜
        print("ã‚¹ãƒ†ãƒƒãƒ—4: Parquetä¿å­˜")
        print("-" * 60)
        success = save_to_parquet(transformed_data, s3_output_path)
        print()
        
        if not success:
            print("âŒ Parquetä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # å®Œäº†
        print("=" * 60)
        print("âœ… ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 60)
        print()
        
        print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ")
        print("2. Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç™»éŒ²")
        print("3. AWS Athenaã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
        print()
        
        print(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«: {s3_output_path}")
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
