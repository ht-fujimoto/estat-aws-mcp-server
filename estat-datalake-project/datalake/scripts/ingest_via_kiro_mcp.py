#!/usr/bin/env python3
"""
Kiro MCPçµ±åˆã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ³¨æ„: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Kiroã®MCPãƒ„ãƒ¼ãƒ«ã‚’ç›´æ¥å‘¼ã³å‡ºã™ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚
å®Ÿéš›ã®å®Ÿè¡Œã¯Kiroç’°å¢ƒå†…ã§è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
"""

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from datalake.dataset_selection_manager import DatasetSelectionManager


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("E-stat ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–ã‚Šè¾¼ã¿ï¼ˆKiro MCPçµ±åˆç‰ˆï¼‰")
    print("=" * 60)
    print()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã‚€
    config_path = project_root / "datalake" / "config" / "dataset_config.yaml"
    dataset_manager = DatasetSelectionManager(str(config_path))
    
    # å–ã‚Šè¾¼ã¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
    pending_datasets = [
        ds for ds in dataset_manager.inventory 
        if ds.get('status') == 'pending'
    ]
    
    if not pending_datasets:
        print("âš ï¸  å–ã‚Šè¾¼ã¿å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆstatus=pendingï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")
        print()
        print("ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
        for ds in dataset_manager.inventory:
            print(f"  - {ds.get('name', ds['id'])} ({ds['id']}): {ds.get('status')}")
        return True
    
    print(f"ğŸ“Š å–ã‚Šè¾¼ã¿å¯¾è±¡: {len(pending_datasets)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    for ds in pending_datasets:
        print(f"  - {ds.get('name', ds['id'])} ({ds['id']})")
    print()
    
    print("=" * 60)
    print("MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿æ‰‹é †")
    print("=" * 60)
    print()
    
    for i, dataset in enumerate(pending_datasets, 1):
        dataset_id = dataset['id']
        dataset_name = dataset.get('name', dataset_id)
        domain = dataset.get('domain', 'generic')
        
        print(f"{i}. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
        print(f"   ID: {dataset_id}")
        print(f"   ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
        print()
        
        print("   å®Ÿè¡Œã™ã‚‹MCPãƒ„ãƒ¼ãƒ«:")
        print(f"   1) mcp_estat_aws_remote_fetch_dataset_auto")
        print(f"      - dataset_id: {dataset_id}")
        print(f"      - convert_to_japanese: true")
        print(f"      - save_to_s3: true")
        print()
        
        print(f"   2) mcp_estat_aws_remote_transform_to_parquet")
        print(f"      - s3_json_path: (ã‚¹ãƒ†ãƒƒãƒ—1ã®å‡ºåŠ›)")
        print(f"      - data_type: {domain}")
        print()
        
        print(f"   3) mcp_estat_aws_remote_load_to_iceberg")
        print(f"      - table_name: {domain}_data")
        print(f"      - s3_parquet_path: (ã‚¹ãƒ†ãƒƒãƒ—2ã®å‡ºåŠ›)")
        print()
    
    print("=" * 60)
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 60)
    print()
    print("Kiroãƒãƒ£ãƒƒãƒˆã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print()
    
    for dataset in pending_datasets:
        dataset_id = dataset['id']
        domain = dataset.get('domain', 'generic')
        print(f"# {dataset.get('name', dataset_id)}")
        print(f"1. ãƒ‡ãƒ¼ã‚¿å–å¾—:")
        print(f"   mcp_estat_aws_remote_fetch_dataset_auto(dataset_id='{dataset_id}', convert_to_japanese=True, save_to_s3=True)")
        print()
        print(f"2. Parquetå¤‰æ›:")
        print(f"   mcp_estat_aws_remote_transform_to_parquet(s3_json_path='<ã‚¹ãƒ†ãƒƒãƒ—1ã®å‡ºåŠ›>', data_type='{domain}')")
        print()
        print(f"3. IcebergæŠ•å…¥:")
        print(f"   mcp_estat_aws_remote_load_to_iceberg(table_name='{domain}_data', s3_parquet_path='<ã‚¹ãƒ†ãƒƒãƒ—2ã®å‡ºåŠ›>')")
        print()
        print("-" * 60)
        print()
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
